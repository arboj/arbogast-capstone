{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Linear Support Vector Machine - Practice\n",
    "\n",
    "In this session, you will practice using Linear SVM on **red wine** dataset\n",
    "with the typical train/validate workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "''' Preporcessing steps: \n",
    "1. lowercasing \n",
    "2. Digit -> DDD \n",
    "3. URLs -> httpAddress \n",
    "4. @username -> userID \n",
    "5. Remove special characters, keep ; . ! ? \n",
    "6. normalize elongation \n",
    "7. tokenization using tweetNLP\n",
    "output is ~/Dropbox (QCRI)/AIDR-DA-ALT-SC/data/labeled datasets/prccd_data/{filename}_AIDR_prccd.csv\n",
    "'''\n",
    "#################################################################\n",
    "\n",
    "#=================\n",
    "#==> Libraries <==\n",
    "#=================\n",
    "import re, os\n",
    "import string \n",
    "import sys\n",
    "import twokenize\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from os.path import basename\n",
    "import ntpath\n",
    "import codecs\n",
    "import unicodedata\n",
    "\n",
    "def process(lst):\n",
    "    prccd_item_list=[]\n",
    "    for tweet in lst:\n",
    "\n",
    "\n",
    "#         # Normalizing utf8 formatting\n",
    "#         tweet = tweet.decode(\"unicode-escape\").encode(\"utf8\").decode(\"utf8\")\n",
    "#         #tweet = tweet.encode(\"utf-8\")\n",
    "#         tweet = tweet.encode(\"ascii\",\"ignore\")\n",
    "#         tweet = tweet.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 1. Lowercasing\n",
    "        tweet = tweet.lower()\n",
    "        #print \"[lowercase]\", tweet\n",
    "\n",
    "        # Word-Level\n",
    "        tweet = re.sub(' +',' ',tweet) # replace multiple spaces with a single space\n",
    "\n",
    "        # 2. Normalizing digits\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if word.isdigit()]:\n",
    "            tweet = tweet.replace(word, \"D\" * len(word))\n",
    "#         print( \"[digits]\", tweet)\n",
    "\n",
    "        # 3. Normalizing URLs\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if '/' in word or '.' in word and  len(word) > 3]:\n",
    "            tweet = tweet.replace(word, \"\")\n",
    "#         print( \"[URLs]\", tweet)\n",
    "\n",
    "        #4. Normalizing username\n",
    "\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        try:\n",
    "            for word in [word for word in tweet_words if word[0] == '@' and len(word) > 1]:\n",
    "                tweet = tweet.replace(word, \"\")\n",
    "#         print( \"[usrename]\", tweet)\n",
    "        except:\n",
    "            tweet = tweet\n",
    "\n",
    "\n",
    "        # 5. Removing special Characters\n",
    "        punc = '@$%^&*()_+-={}[]:\"|\\'\\~`<>/,'\n",
    "        trans = str.maketrans(punc, ' '*len(punc))\n",
    "        tweet = tweet.translate(trans)\n",
    "        #print( \"[punc]\", tweet)\n",
    "\n",
    "        # 6. Normalizing +2 elongated char\n",
    "        tweet = re.sub(r\"(.)\\1\\1+\",r'\\1\\1', tweet)\n",
    "        #print (\"[elong]\", tweet)\n",
    "\n",
    "        # 7. tokenization using tweetNLP\n",
    "        tweet = ' '.join(twokenize.simpleTokenize(tweet))\n",
    "        #print( \"[token]\", tweet )\n",
    "\n",
    "        #8. fix \\n char\n",
    "        tweet = tweet.replace('\\n', ' ')\n",
    "\n",
    "        prccd_item_list.append(tweet.strip())\n",
    "#         print (\"[processed]\", tweet.replace('\\n', ' '))\n",
    "        \n",
    "    return prccd_item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Elements/DataScience/dsa/capstone\n",
      "processing: combinedf\n",
      "processing: combinedf complete\n",
      "processing: train\n",
      "processing: train complete\n",
      "processing: test\n",
      "processing: test complete\n",
      "processing: dev\n",
      "processing: dev complete\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# Loading\n",
    "######################\n",
    "\n",
    "code_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "print(parent_dir)\n",
    "labled_data_folder  =  os.path.join(parent_dir,\"Data/crisis_datasets_benchmarks/all_data_en\")\n",
    "initial_filtering_folder = os.path.join(parent_dir,\"Data/crisis_datasets_benchmarks/initial_filtering\")\n",
    "\n",
    "\n",
    "train = pd.read_table(os.path.join\n",
    "                       (labled_data_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang_en_train.tsv\"))\n",
    "test = pd.read_table(os.path.join\n",
    "                       (labled_data_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang_en_test.tsv\"),\n",
    "                       sep ='\\t', quoting =3)\n",
    "dev = pd.read_table(os.path.join\n",
    "                     (labled_data_folder,\n",
    "                                  \"crisis_consolidated_informativeness_filtered_lang_en_dev.tsv\"))\n",
    "combinedf = pd.concat([train,test,dev])\n",
    "df_list = [combinedf,train, test, dev]\n",
    "df_list_name = ['combinedf','train', 'test', 'dev']\n",
    "proc_list = []\n",
    "for i, df in enumerate(df_list):\n",
    "    print(\"processing: \"+df_list_name[i])\n",
    "    df['processed_txt'] = process(df['text'])\n",
    "    print(\"processing: \"+df_list_name[i] + \" complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "In the following cell, **print out class distribution** before and after labels are binarized respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Pull features and labels\n",
    "selected_features = [1,6,9,10]\n",
    "X = scale(np.array(dataset.iloc[:, selected_features]))\n",
    "y = np.array(dataset.quality)\n",
    "\n",
    "# Complete code below this comment  (Question #P6001)\n",
    "# ----------------------------------\n",
    "print('Class distribution (before binarization):', {i: ['classes'] for i in np.unique(y)})\n",
    "\n",
    "# Binarize labels\n",
    "y = y>=6\n",
    "print('Class distribution (after binarization):', {i: ['classes'] for i in np.unique(y)})\n",
    "# ----------------------------------\n",
    "\n",
    "# Create training/validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create linear SVM model\n",
    "\n",
    "Collect names for all selected feature columns."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_names = [dataset.columns[i].replace('\\x20', '_') for i in selected_features]\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create feature columns placeholders for TensorFlow SVM."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Complete code below this comment  (Question #P6002)\n",
    "# ----------------------------------\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(feature_name) for feature_name in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #P6003)\n",
    "# ----------------------------------\n",
    "classifier = tf.contrib.learn.SVM('example_id', feature_columns=feature_columns, l2_regularization=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and preparation\n",
    "\n",
    "Create input_fn() to supply training data for linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #P6004)\n",
    "# ----------------------------------\n",
    "def input_fn():\n",
    "    columns = {\n",
    "        feature_name: tf.constant(np.expand_dims(X_train[:,i], 1))\n",
    "            for i,feature_name in enumerate(feature_names)\n",
    "    }\n",
    "    columns['example_id'] = tf.constant([str(i+1) for i in range(len(X_train))])\n",
    "    labels = tf.constant(y_train)\n",
    "    return columns, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVM(params={'feature_columns': [_RealValuedColumn(column_name='volatile_acidity', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='total_sulfur_dioxide', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='sulphates', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), _RealValuedColumn(column_name='alcohol', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)], 'weight_column_name': None, 'head': <tensorflow.contrib.learn.python.learn.estimators.head._BinarySvmHead object at 0x7fa8efec4e10>, 'optimizer': <tensorflow.contrib.linear_optimizer.python.sdca_optimizer.SDCAOptimizer object at 0x7fa8efec44a8>, 'update_weights_hook': <tensorflow.contrib.learn.python.learn.estimators.linear._SdcaUpdateWeightsHook object at 0x7fa8efec41d0>})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  (Question #P6005)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "classifier.fit(input_fn=input_fn, steps=30)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Create a predict_fn() to supply data to make predictions.  \n",
    "Then call classifier.predict() to create y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete code below this comment  (Question #P6006)\n",
    "# ----------------------------------\n",
    "def predict_fn():\n",
    "    columns = {\n",
    "        feature_name: tf.constant(np.expand_dims(X_test[:,i], 1))\n",
    "            for i,feature_name in enumerate(feature_names)\n",
    "    }\n",
    "    columns['example_id'] = tf.constant([str(i+1) for i in range(len(X_test))])\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call classifier.predict() to create **y_pred** as predictions.\n",
    "\n",
    "**Hint**: See LinearSVM lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #P6007)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "y_pred=classifier.predict(input_fn=predict_fn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = list(map(lambda i: i['classes'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed predictions **y_pred** along with ground truth **y_test** to confusion_matrix() to create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function confusion_matrix in module sklearn.metrics.classification:\n",
      "\n",
      "confusion_matrix(y_true, y_pred, labels=None, sample_weight=None)\n",
      "    Compute confusion matrix to evaluate the accuracy of a classification\n",
      "    \n",
      "    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "    is equal to the number of observations known to be in group :math:`i` but\n",
      "    predicted to be in group :math:`j`.\n",
      "    \n",
      "    Thus in binary classification, the count of true negatives is\n",
      "    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array, shape = [n_samples]\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : array, shape = [n_samples]\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array, shape = [n_classes], optional\n",
      "        List of labels to index the matrix. This may be used to reorder\n",
      "        or select a subset of labels.\n",
      "        If none is given, those that appear at least once\n",
      "        in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "    \n",
      "    sample_weight : array-like of shape = [n_samples], optional\n",
      "        Sample weights.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    C : array, shape = [n_classes, n_classes]\n",
      "        Confusion matrix\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] `Wikipedia entry for the Confusion matrix\n",
      "           <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import confusion_matrix\n",
      "    >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "    >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "    >>> confusion_matrix(y_true, y_pred)\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "    \n",
      "    >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "    >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "    >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "    \n",
      "    In the binary case, we can extract true positives, etc as follows:\n",
      "    \n",
      "    >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "    >>> (tn, fp, fn, tp)\n",
      "    (0, 2, 1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158,  46],\n",
       "       [ 54, 142]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  (Question #P6008)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
