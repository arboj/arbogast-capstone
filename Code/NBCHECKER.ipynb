{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074525a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul 13 22:15:15 2021\n",
    "\n",
    "@author: Arbo\n",
    "\"\"\"\n",
    "import re\n",
    "from string import punctuation                   # to extract the puntuation symbols\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize          # to divide strings into tokens\n",
    "from nltk.stem import WordNetLemmatizer          # to lemmatize the tokens\n",
    "from nltk.corpus import stopwords                # to remove the stopwords \n",
    "import numpy as np\n",
    "import tensorflow as tf            \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    #text=str(text)\n",
    "    text = text.lower()\n",
    "    #get rid of usernames\n",
    "    tweet_words = text.strip('\\r').split(' ')\n",
    "    for word in [word for word in tweet_words if '@' in word]:\n",
    "            \n",
    "            text = text.replace(word, \"\")\n",
    "    #get rid of the re-tweet\n",
    "    tweet_words = text.strip('\\r').split(' ')\n",
    "    for word in [word for word in tweet_words if 'rt' == word]:\n",
    "            \n",
    "            text = text.replace(word, \"\")\n",
    "            \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]  ##Notice the use of text.\n",
    "\n",
    "def concatenate_text(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "def makeglove (path_to_glove_file):\n",
    "    embeddings_index = {}\n",
    "    f = open(path_to_glove_file, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]                                  # the first entry is the word\n",
    "        coefs = np.asarray(splitLine[1:], dtype='float32')   # these are the vectors representing word embeddings\n",
    "        embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def make_embedding_matrix(train_samples, val_samples, embeddings_index):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function computes the embedding matrix that will be used in the embedding layer\n",
    "    \n",
    "    Parameters:\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        embeddings_index: Python dictionary with word embeddings\n",
    "    \n",
    "    Returns:\n",
    "        embedding_matrix: embedding matrix with the dimensions (num_tokens, embedding_dim), \n",
    "        where num_tokens is the vocabulary of the input data, \n",
    "        and emdebbing_dim is the number of components in the GloVe vectors (can be 50,100,200,300)\n",
    "        vectorizer: TextVectorization layer      \n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = TextVectorization(max_tokens=55000, output_sequence_length=50)\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "    vectorizer.adapt(text_ds)\n",
    "    \n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "      \n",
    "    num_tokens = len(voc)\n",
    "    \n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "#   creating an embedding matrix\n",
    "    embedding_dim = len(embeddings_index['the'])\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "\n",
    "#     print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    print(f\"Converted {hits} words ({misses} misses).\")\n",
    "\n",
    "    return embedding_matrix, vectorizer\n",
    "\n",
    "def processme(twts):\n",
    "    procstart =  datetime.datetime.now()\n",
    "    print(\"{}: processing \".format(procstart))\n",
    "    twts['ptext'] = twts['Text'].apply(lambda x: clean_text(x))\n",
    "    twts['ptext'] = twts['ptext'].apply(lambda x: word_tokenize(x))\n",
    "    twts['ptext'] = twts['ptext'].apply(lambda x : remove_stopwords(x))\n",
    "    twts['ptext'] = twts['ptext'].apply(lambda x : lemmatize_text(x))\n",
    "    twts['ptext'] = twts['ptext'].apply(lambda x : concatenate_text(x))\n",
    "    procend=  datetime.datetime.now() \n",
    "    print(\"{}: processed in {}\".format(procend, procend - procstart))\n",
    "    return twts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b780e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul 13 22:39:38 2021\n",
    "\n",
    "@author: Arbo\n",
    "\"\"\"\n",
    "from mordecai import Geoparser\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import tensorflow as tf            \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_val_split(df, validation_split):\n",
    "    \"\"\"\n",
    "    This function generates the training and validation splits from an input dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe: pandas dataframe with columns \"text\" and \"target\" (binary)\n",
    "        validation_split: should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the validation split\n",
    "    \n",
    "    Returns:\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        train_labels: list of labels (0 or 1) in the training dataset\n",
    "        val_labels: list of labels (0 or 1) in the validation dataset      \n",
    "    \"\"\"\n",
    "       \n",
    "    text = df['text'].values.tolist()                         # input text as list\n",
    "    targets = df['class_label_cat'].values.tolist()                    # targets\n",
    "    \n",
    "#   Preparing the training/validation datasets\n",
    "    \n",
    "    seed = random.randint(1,50)   # random integer in a range (1, 50)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(text)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(targets)\n",
    "\n",
    "    num_validation_samples = int(validation_split * len(text))\n",
    "\n",
    "    train_samples = text[:-num_validation_samples]\n",
    "    val_samples = text[-num_validation_samples:]\n",
    "    train_labels = targets[:-num_validation_samples]\n",
    "    val_labels = targets[-num_validation_samples:]\n",
    "    \n",
    "    print(f\"Total size of the dataset: {df.shape[0]}.\")\n",
    "    print(f\"Training dataset: {len(train_samples)}.\")\n",
    "    print(f\"Validation dataset: {len(val_samples)}.\")\n",
    "    \n",
    "    return train_samples, val_samples, train_labels, val_labels\n",
    "\n",
    "def test_listerine(df):\n",
    "    \"\"\"\n",
    "    This function generates the test x and y from an input dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe: pandas dataframe with columns \"text\" and \"class_label_cat\" (binary)\n",
    "        \n",
    "    \n",
    "    Returns:\n",
    "        test_samples: list of strings in the training dataset\n",
    "\n",
    "        test_labels: list of labels (0 or 1) in the training dataset\n",
    "    \n",
    "    \"\"\"\n",
    "       \n",
    "    text = df['text'].values.tolist()                         # input text as list\n",
    "    targets = df['class_label_cat'].values.tolist()                    # targets\n",
    "    \n",
    "#   Preparing the training/validation datasets\n",
    "    \n",
    "    seed = random.randint(1,50)   # random integer in a range (1, 50)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(text)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(targets)\n",
    "\n",
    "   \n",
    "\n",
    "    test_samples = text\n",
    "   \n",
    "    test_labels = targets\n",
    "    \n",
    "    \n",
    "    print(f\"Total size of the dataset: {df.shape[0]}.\")\n",
    "\n",
    "    \n",
    "    return test_samples, test_labels\n",
    "\n",
    "\n",
    "def geo_df(df,geo):\n",
    "    \n",
    "# =============================================================================\n",
    "#     geo = Geoparser()\n",
    "# =============================================================================\n",
    "    df['geos'] = geo.batch_geoparse(df['Text'])\n",
    "    df_geo = df[df[\"geos\"].str.len() != 0]\n",
    "    df_geo = df_geo.explode('geos')\n",
    "    df_geo = pd.concat([df_geo.drop(['geos'], axis=1), df_geo['geos'].apply(pd.Series)], axis=1)\n",
    "    df_geo = pd.concat([df_geo.drop(['geo'], axis=1), df_geo['geo'].apply(pd.Series)], axis=1)\n",
    "    df_geo = df_geo[df_geo['lat'].notnull()]\n",
    "    df_geo.lat = df_geo.lat.astype(float)\n",
    "    df_geo.lon =df_geo.lon.astype(float)\n",
    "    return df_geo\n",
    "\n",
    "#     df_js = pd.DataFrame()\n",
    "#     for row in range(len(result_inf)):\n",
    "#         df_temp = pd.json_normalize(result_inf['geos'], record_path =['spans'], \n",
    "#         meta=['word',\"country_predicted\", \"country_conf\",['geo',\"admin1\"],\n",
    "#               ['geo',\"lat\"],['geo',\"lon\"],['geo',\"country_code3\"],['geo',\"geonameid\"],\n",
    "#               ['geo',\"place_name\"],['geo',\"feature_class\"],['geo',\"feature_code\"]],\n",
    "#         errors='ignore'\n",
    "#     )\n",
    "#         df_temp['TweetId']=''\n",
    "#         for i in range(len(df_temp)):\n",
    "#             df_temp['TweetId'][i]=tweets_df['TweetId'][row]\n",
    "#         df_js=df_js.append(df_temp,ignore_index=True)\n",
    "\n",
    "#     df_js = df_js.rename(columns = {'TweetId':'TweetId', 'start':'start', 'end':'end', \n",
    "#                                     'word':'word','country_predicted':'country_predicted', \n",
    "#                                     'country_conf': 'country_conf','geo.admin1':'admin1', \n",
    "#                                     'geo.lat':'lat', 'geo.lon':'lon', \n",
    "#                                     'geo.country_code3':'country_code3','geo.geonameid':'geonameid', \n",
    "#                                     'geo.place_name':'place_name', \n",
    "#                                     'geo.feature_class':'feature_class','geo.feature_code':'feature_code'})\n",
    "#     return df_js\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def suggest_nn2(df, model, vectorizer):\n",
    "    \"\"\"\n",
    "    This function generates (binary) targets from a dataframe with column \"text\" using trained Keras model\n",
    "    \n",
    "    Parameters:\n",
    "        df: pandas dataframe with column \"text\"\n",
    "        model: Keras model (trained)\n",
    "    \n",
    "    Output:\n",
    "        predictions: list of suggested targets corresponding to string entries from the column \"text\"\n",
    "    \"\"\"\n",
    "    \n",
    "    string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorizer(string_input)\n",
    "    preds = model(x)\n",
    "    end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "    probabilities = end_to_end_model.predict(df[\"ptext\"])\n",
    "    \n",
    "    predictions = [1 if i > 0.5 else 0 for i in probabilities]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def suggest_nn3(df, model, vectorizer):\n",
    "    \"\"\"\n",
    "    This function generates (binary) targets from a dataframe with column \"text\" using trained Keras model\n",
    "    \n",
    "    Parameters:\n",
    "        df: pandas dataframe with column \"text\"\n",
    "        model: Keras model (trained)\n",
    "    \n",
    "    Output:\n",
    "        predictions: list of suggested targets corresponding to string entries from the column \"text\"\n",
    "    \"\"\"\n",
    "    \n",
    "    string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorizer(string_input)\n",
    "    preds = model(x)\n",
    "    end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "    probabilities = end_to_end_model.predict(df[\"text\"])\n",
    "    \n",
    "    predictions = [1 if i > 0.5 else 0 for i in probabilities]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def initialize_nn(embedding_matrix):\n",
    "    \"\"\"\n",
    "    This function initializes Keras model for binary text classification\n",
    "    \n",
    "    Parameters:\n",
    "        embedding matrix with the dimensions (num_tokens, embedding_dim),\n",
    "         where num_tokens is the vocabulary size of the input data,\n",
    "          and emdebbing_dim is the number of components in the GloVe vectors\n",
    "    \n",
    "    Returns:\n",
    "        model: Keras model    \n",
    "    \"\"\"\n",
    "    \n",
    "    num_tokens = embedding_matrix.shape[0]\n",
    "    embedding_dim = embedding_matrix.shape[1]\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,                # we are not going to train the embedding vectors\n",
    "    )\n",
    "    \n",
    "#   Here we define the architecture of the Keras model. \n",
    "    int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    x = embedding_layer(int_sequences_input) \n",
    "    x = layers.Dropout(.7)(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128,                                        \n",
    "                                          dropout=.4,\n",
    "                                          return_sequences=True))(x)\n",
    "# =============================================================================\n",
    "#     x = layers.Bidirectional(layers.LSTM(32,\n",
    "#                                           dropout=.5))(x)\n",
    "# =============================================================================\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Dropout(.5)(x)\n",
    "    preds = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_nn(model, train_samples, val_samples, train_labels, val_labels, vectorizer, stop = True):\n",
    "    \"\"\"\n",
    "    This function fits the training data using validation data to calculate metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        model: preinitialized Keras model\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        train_labels: list of labels (0 or 1) in the training dataset\n",
    "        val_labels: list of labels (0 or 1) in the validation dataset\n",
    "        vectorizer: TextVectorization layer\n",
    "        stop (Boolean): flag for Early Stopping (aborting training when a monitored metric has stopped improving)\n",
    "    \n",
    "    Returns:\n",
    "        model: trained Keras model\n",
    "        history: callback that can be used to track the learning process\n",
    "    \"\"\"\n",
    "    \n",
    "    print('')\n",
    "    print(\"Training the model...\")\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=\"adam\", \n",
    "              metrics=[\"binary_accuracy\"])\n",
    "    \n",
    "    x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "    x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "    \n",
    "    y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "    y_val = np.asarray(val_labels).astype('float32').reshape((-1,1))\n",
    "\n",
    "    \n",
    "    \n",
    "    if stop:\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "        history = model.fit(x_train, y_train, batch_size=32, epochs=50, validation_data=(x_val, y_val), callbacks=[early_stopping], verbose=1)\n",
    "    else:\n",
    "        history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val), verbose=1)\n",
    "        \n",
    "    return model, history\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c987dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-11 17:54:43.173678: modules loaded in 0:00:00.004797 loading data\n",
      "Current working directory: /Volumes/Elements/DataScience/dsa/capstone\n",
      "C working directory: /Volumes/Elements/DataScience/dsa/capstone\n",
      "2021-08-11 17:54:43.173678: data loaded in 0:12:35.725148 cleaning text elapsed: 0:12:35.729945\n",
      "2021-08-11 18:25:43.805174: text cleand in -1 day, 23:41:35.093652 loading vector and spliting samples elapsed: 0:31:00.636293\n",
      "Total size of the dataset: 117339.\n",
      "Training dataset: 88005.\n",
      "Validation dataset: 29334.\n",
      "indexing\n",
      "2021-08-11 18:26:47.999284: indexed 0:01:04.194110 matrix and vectorize ellapsed 0:32:04.830403\n",
      "Converted 32297 words (22703 misses).\n",
      "2021-08-11 18:26:53.137032: mtx'd at vct'd in 0:00:05.137748 loading variables as vectors ellapsed 0:32:09.968151\n",
      "2021-08-11 18:27:08.600511: training data ready in 0:00:15.463479 buildinf initial mod  ellapsed 0:32:25.431630\n"
     ]
    }
   ],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Created on Tue Aug 10 23:46:58 2021\n",
    "\n",
    "# @author: Arbo\n",
    "# \"\"\"\n",
    "# import datetime\n",
    "# overallstart = datetime.datetime.now() \n",
    "# print (\"started at: {}\".format(overallstart))\n",
    "\n",
    "# import numpy as np                               # linear algebra\n",
    "# import pandas as pd                              # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import os\n",
    "# import re                                        # to handle regular expressions\n",
    "# from string import punctuation                   # to extract the puntuation symbols\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize          # to divide strings into tokens\n",
    "# from nltk.stem import WordNetLemmatizer          # to lemmatize the tokens\n",
    "# from nltk.corpus import stopwords                # to remove the stopwords \n",
    "\n",
    "# import random                                    # for generating (pseudo-)random numbers\n",
    "# import matplotlib.pyplot as plt                  # to plot some visualizations\n",
    "\n",
    "# import tensorflow as tf            \n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# from tensorflow.keras.layers import Embedding\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# from tensorflow.keras import layers\n",
    "# # =============================================================================\n",
    "# # from kerastuner.tuners import RandomSearch\n",
    "# # from kerastuner.tuners import BayesianOptimization\n",
    "# # from kerastuner.tuners import Hyperband\n",
    "# # import kerastuner as kt\n",
    "# # =============================================================================\n",
    "\n",
    "\n",
    "# # import text_proccessing\n",
    "# # from text_proccessing import clean_text\n",
    "# # from text_proccessing import remove_stopwords\n",
    "# # from text_proccessing import lemmatize_text\n",
    "# # from text_proccessing import concatenate_text\n",
    "# # from text_proccessing import makeglove\n",
    "# # from text_proccessing import make_embedding_matrix\n",
    "# # from modhelp import train_val_split\n",
    "# # from modhelp import test_listerine\n",
    "# # from modhelp import suggest_nn3\n",
    "# # from modhelp import initialize_nn\n",
    "# # from modhelp import  train_nn\n",
    "\n",
    "# starti = datetime.datetime.now() \n",
    "print(\"{}: modules loaded in {} loading data\".format(starti, starti-overallstart))\n",
    "code_dir = os.getcwd()\n",
    "print(\"Current working directory: {0}\".format(code_dir))\n",
    "parent_dir = os.getcwd()\n",
    "print(\"C working directory: {0}\".format(code_dir))\n",
    "data_dir = os.path.join(parent_dir,\"Data\")\n",
    "tweet_dir = os.path.join(parent_dir,\"TweetMap\")\n",
    "directory = os.path.join(data_dir,'splits')\n",
    "dsa= os.path.dirname(code_dir)\n",
    "# model = tf.keras.models.load_model(os.path.join(dsa,'model1'))\n",
    "train_data = pd.read_csv(os.path.join(directory, 'InformativenessTrain_Processed.csv'))\n",
    "test_data  = pd.read_csv(os.path.join(directory, 'InformativenessTest_Processed.csv'))\n",
    "dtl = datetime.datetime.now()\n",
    "print(\"{}: data loaded in {} cleaning text elapsed: {}\".format(starti,dtl-starti,dtl-overallstart))\n",
    "\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x: clean_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: clean_text(x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x:word_tokenize(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x:word_tokenize(x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x : remove_stopwords(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : remove_stopwords(x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x : lemmatize_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : lemmatize_text(x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x : concatenate_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : concatenate_text(x))\n",
    "\n",
    "step1 = datetime.datetime.now()\n",
    "print(\"{}: text cleand in {} loading vector and spliting samples elapsed: {}\".format(step1,dtl-step1, step1-overallstart))\n",
    "path_to_glove_file = os.path.join(dsa,'WordVector','glove.twitter.27B.200d.txt')\n",
    "train_samples, val_samples, train_labels, val_labels = train_val_split(train_data, 0.25)\n",
    "# =============================================================================\n",
    "# test_samples, test_labels = test_listerine(test_data)\n",
    "# =============================================================================\n",
    "print(\"indexing\")\n",
    "embeddings_index=makeglove(path_to_glove_file)\n",
    "step2 = datetime.datetime.now()\n",
    "print(\"{}: indexed {} matrix and vectorize ellapsed {}\".format(step2, step2-step1, step2-overallstart))\n",
    "embedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)\n",
    "step3 = datetime.datetime.now()\n",
    "print(\"{}: mtx'd at vct'd in {} loading variables as vectors ellapsed {}\".format(step3, step3-step2,  step3-overallstart))\n",
    "\n",
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "# =============================================================================\n",
    "# x_test = vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
    "# =============================================================================\n",
    "y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "y_val = np.asarray(val_labels).astype('float32').reshape((-1,1))\n",
    "# =============================================================================\n",
    "# y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))\n",
    "# =============================================================================\n",
    "step4 = datetime.datetime.now()\n",
    "print(\"{}: training data ready in {} buildinf initial mod  ellapsed {}\".format(step4, step4-step3,  step4-overallstart))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = suggest_nn3(test_data, model,vectorizer)\n",
    "\n",
    "submission_data = {\"ID\": test_data['id'].tolist(),\"tweet\": test_data['text'].tolist(), \"target\": predictions}\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "result = test_data.join(submission_df.target)\n",
    "\n",
    "result.to_csv(os.path.join(tweet_dir,\"dubblecheck\"),index =False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b428532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>lang_confidence</th>\n",
       "      <th>class_label</th>\n",
       "      <th>class_label_cat</th>\n",
       "      <th>processed_txt</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>368967734542864384</td>\n",
       "      <td>2013_manila_floods</td>\n",
       "      <td>crisislext26</td>\n",
       "      <td>pagasa yellow advisory metro manila moderatehe...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>informative</td>\n",
       "      <td>0</td>\n",
       "      <td>pagasa 12 20pm yellow advisory for metro moder...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>369089159635283968</td>\n",
       "      <td>2013_manila_floods</td>\n",
       "      <td>crisislext26</td>\n",
       "      <td>pagasa red rainfall warning weather system sou...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.72282</td>\n",
       "      <td>informative</td>\n",
       "      <td>0</td>\n",
       "      <td>pagasa red rainfall warning no D weather syste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>369122655359598592</td>\n",
       "      <td>2013_manila_floods</td>\n",
       "      <td>crisislext26</td>\n",
       "      <td>tropical storm maring continue enhance southwe...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>informative</td>\n",
       "      <td>0</td>\n",
       "      <td>tropical storm maring will continue to enhance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>369139034133524480</td>\n",
       "      <td>2013_manila_floods</td>\n",
       "      <td>crisislext26</td>\n",
       "      <td>cant sleep rain maringph ughh</td>\n",
       "      <td>en</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>informative</td>\n",
       "      <td>0</td>\n",
       "      <td>can t sleep because of this rain #maringph #ughh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>369142238564974592</td>\n",
       "      <td>2013_manila_floods</td>\n",
       "      <td>crisislext26</td>\n",
       "      <td>well bad u heard news already waistdeep flood ...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>informative</td>\n",
       "      <td>0</td>\n",
       "      <td>well it s not that bad for us here but i heard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id               event        source  \\\n",
       "0  368967734542864384  2013_manila_floods  crisislext26   \n",
       "1  369089159635283968  2013_manila_floods  crisislext26   \n",
       "2  369122655359598592  2013_manila_floods  crisislext26   \n",
       "3  369139034133524480  2013_manila_floods  crisislext26   \n",
       "4  369142238564974592  2013_manila_floods  crisislext26   \n",
       "\n",
       "                                                text lang  lang_confidence  \\\n",
       "0  pagasa yellow advisory metro manila moderatehe...   en          1.00000   \n",
       "1  pagasa red rainfall warning weather system sou...   en          0.72282   \n",
       "2  tropical storm maring continue enhance southwe...   en          1.00000   \n",
       "3                      cant sleep rain maringph ughh   en          1.00000   \n",
       "4  well bad u heard news already waistdeep flood ...   en          1.00000   \n",
       "\n",
       "   class_label  class_label_cat  \\\n",
       "0  informative                0   \n",
       "1  informative                0   \n",
       "2  informative                0   \n",
       "3  informative                0   \n",
       "4  informative                0   \n",
       "\n",
       "                                       processed_txt  target  \n",
       "0  pagasa 12 20pm yellow advisory for metro moder...       0  \n",
       "1  pagasa red rainfall warning no D weather syste...       0  \n",
       "2  tropical storm maring will continue to enhance...       0  \n",
       "3   can t sleep because of this rain #maringph #ughh       0  \n",
       "4  well it s not that bad for us here but i heard...       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3192029a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23432,     0],\n",
       "       [    0, 15681]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = result.class_label_cat\n",
    "y_pred = result.class_label_cat\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f2427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
