{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0543eb34",
   "metadata": {},
   "source": [
    "# Text Conditioning and Machine Learning\n",
    "\n",
    "This notebook lays out the process of text analysis and machine learning of scraped Twitter data to identify tweets relevant to humanitarian crises. First labled twitter data collected from previous natural disaters will be cleaned and vectorized. A XXX model will be trained and tweaked over the data. This model will be saved, then applied over tweets from contemporary humanitarian crises to identify emerging disaster hotspots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1859dc",
   "metadata": {},
   "source": [
    "## Python Packages Used\n",
    "This notebook was set up in an environment running Python 3.8 with the following packages:\n",
    "pandas, tensorflow, keras, scikit-learn, nltk, gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b272dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import collections as col\n",
    "import pprint\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import twokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec2339b",
   "metadata": {},
   "source": [
    "## Preprocessing Script\n",
    "\n",
    "The following script was adapted from the \"preprocessing.py\" script avlible from [The CrisisNLP\n",
    "/deep-learning-for-big-crisis-data GitHub repository](https://github.com/CrisisNLP/deep-learning-for-big-crisis-data).\n",
    "\n",
    "The researchers in their paper mentioned getting rid of the urls, digits, and usersnames improved their nlp results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fab92f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "''' Preporcessing steps: \n",
    "1. lowercasing \n",
    "2. Digit -> DDD \n",
    "3. URLs -> httpAddress \n",
    "4. @username -> userID \n",
    "5. Remove special characters, keep ; . ! ? \n",
    "6. normalize elongation \n",
    "7. tokenization using tweetNLP\n",
    "output is ~/Dropbox (QCRI)/AIDR-DA-ALT-SC/data/labeled datasets/prccd_data/{filename}_AIDR_prccd.csv\n",
    "'''\n",
    "#################################################################\n",
    "\n",
    "#=================\n",
    "#==> Libraries <==\n",
    "#=================\n",
    "import re, os\n",
    "import string \n",
    "import sys\n",
    "import twokenize\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from os.path import basename\n",
    "import ntpath\n",
    "import codecs\n",
    "import unicodedata\n",
    "\n",
    "def process(lst):\n",
    "    prccd_item_list=[]\n",
    "    for tweet in lst:\n",
    "\n",
    "\n",
    "#         # Normalizing utf8 formatting\n",
    "#         tweet = tweet.decode(\"unicode-escape\").encode(\"utf8\").decode(\"utf8\")\n",
    "#         #tweet = tweet.encode(\"utf-8\")\n",
    "#         tweet = tweet.encode(\"ascii\",\"ignore\")\n",
    "#         tweet = tweet.strip(' \\t\\n\\r')\n",
    "\n",
    "        # 1. Lowercasing\n",
    "        tweet = tweet.lower()\n",
    "        #print \"[lowercase]\", tweet\n",
    "\n",
    "        # Word-Level\n",
    "        tweet = re.sub(' +',' ',tweet) # replace multiple spaces with a single space\n",
    "\n",
    "        # 2. Normalizing digits\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if word.isdigit()]:\n",
    "            tweet = tweet.replace(word, \"D\" * len(word))\n",
    "#         print( \"[digits]\", tweet)\n",
    "\n",
    "        # 3. Normalizing URLs\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        for word in [word for word in tweet_words if '/' in word or '.' in word and  len(word) > 3]:\n",
    "            tweet = tweet.replace(word, \"\")\n",
    "#         print( \"[URLs]\", tweet)\n",
    "\n",
    "        #4. Normalizing username\n",
    "\n",
    "        tweet_words = tweet.strip('\\r').split(' ')\n",
    "        try:\n",
    "            for word in [word for word in tweet_words if word[0] == '@' and len(word) > 1]:\n",
    "                tweet = tweet.replace(word, \"\")\n",
    "#         print( \"[usrename]\", tweet)\n",
    "        except:\n",
    "            tweet = tweet\n",
    "\n",
    "\n",
    "        # 5. Removing special Characters\n",
    "        punc = '@$%^&*()_+-={}[]:\"|\\'\\~`<>/,'\n",
    "        trans = str.maketrans(punc, ' '*len(punc))\n",
    "        tweet = tweet.translate(trans)\n",
    "        #print( \"[punc]\", tweet)\n",
    "\n",
    "        # 6. Normalizing +2 elongated char\n",
    "        tweet = re.sub(r\"(.)\\1\\1+\",r'\\1\\1', tweet)\n",
    "        #print (\"[elong]\", tweet)\n",
    "\n",
    "        # 7. tokenization using tweetNLP\n",
    "        tweet = ' '.join(twokenize.simpleTokenize(tweet))\n",
    "        #print( \"[token]\", tweet )\n",
    "\n",
    "        #8. fix \\n char\n",
    "        tweet = tweet.replace('\\n', ' ')\n",
    "\n",
    "        prccd_item_list.append(tweet.strip())\n",
    "#         print (\"[processed]\", tweet.replace('\\n', ' '))\n",
    "        \n",
    "    return prccd_item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54b82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f8dc5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Elements/DataScience/dsa/capstone\n"
     ]
    }
   ],
   "source": [
    "code_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a19dc9",
   "metadata": {},
   "source": [
    "## [Crisis Benchmark data for training Models](https://crisisnlp.qcri.org/crisis_datasets_benchmarks.html)\n",
    "\n",
    "<p>The crisis benchmark dataset consists data from several different data sources such as CrisisLex (<a href=\"http://crisislex.org/data-collections.html#CrisisLexT26\" target=\"_blank\">CrisisLex26</a>, <a href=\"http://crisislex.org/data-collections.html#CrisisLexT6\" target=\"_blank\">CrisisLex6</a>), <a href=\"https://crisisnlp.qcri.org/lrec2016/lrec2016.html\" target=\"_blank\">CrisisNLP</a>, <a href=\"http://mimran.me/papers/imran_shady_carlos_fernando_patrick_practical_2013.pdf\" target=\"_blank\">SWDM2013</a>, <a href=\"http://mimran.me/papers/imran_shady_carlos_fernando_patrick_iscram2013.pdf\" target=\"_blank\">ISCRAM13</a>, Disaster Response Data (DRD), <a href=\"https://data.world/crowdflower/disasters-on-social-media\" target=\"_blank\">Disasters on Social Media (DSM)</a>, <a href=\"https://crisisnlp.qcri.org/crisismmd\" target=\"_blank\">CrisisMMD</a> and data from <a href=\"http://aidr.qcri.org/\" target=\"_blank\">AIDR</a>. \n",
    "\t  The purpose of this work was to map the class label, remove duplicates and provide a benchmark results for the community. </p>\n",
    "\n",
    "The authors have their model and data availible on github at <a href=\"https://github.com/firojalam/crisis_datasets_benchmarks\">https://github.com/firojalam/crisis_datasets_benchmarks</a>    </p>\n",
    "\n",
    "#### Data Availible from: https://crisisnlp.qcri.org/data/crisis_datasets_benchmarks/crisis_datasets_benchmarks_v1.0.tar.gz\n",
    "<h4><strong>References</strong></h4>\n",
    "<ol>\n",
    "<li><a href=\"http://sites.google.com/site/firojalam/\">Firoj Alam</a>, <a href=\"https://hsajjad.github.io/\">Hassan Sajjad</a>, <a href=\"http://mimran.me/\">Muhammad Imran</a> and <a href=\"https://sites.google.com/site/ferdaofli/\">Ferda Ofli</a>, <a href=\"https://arxiv.org/abs/2004.06774\" target=\"_blank\"><strong>CrisisBench: Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing,</strong></a> In ICWSM, 2021. [<a href=\"crisis_dataset_bib1.html\">Bibtex</a>]\n",
    "        </li>\n",
    "<!-- <li><a href=\"http://sites.google.com/site/firojalam/\">Firoj Alam</a>, <a href=\"https://hsajjad.github.io/\">Hassan Sajjad</a>, <a href=\"http://mimran.me/\">Muhammad Imran</a> and <a href=\"https://sites.google.com/site/ferdaofli/\">Ferda Ofli</a>, <a href=\"https://arxiv.org/abs/2004.06774\" target=\"_blank\"><strong>Standardizing and Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing,</strong></a> In arxiv, 2020. [<a href=\"crisis_dataset_bib.html\">Bibtex</a>]</li>-->\n",
    "        <li>Firoj Alam, Ferda Ofli and Muhammad Imran. CrisisMMD: Multimodal Twitter Datasets from Natural Disasters. In Proceedings of the International AAAI Conference on Web and Social Media (ICWSM), 2018, Stanford, California, USA.</li>\n",
    "        <li>Muhammad Imran, Prasenjit Mitra, and Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portorož, Slovenia.</li>\n",
    "        <li>A. Olteanu, S. Vieweg, C. Castillo. 2015. What to Expect When the Unexpected Happens: Social Media Communications Across Crises. In Proceedings of the ACM 2015 Conference on Computer Supported Cooperative Work and Social Computing (CSCW '15). ACM, Vancouver, BC, Canada.</li>\n",
    "        <li>A. Olteanu, C. Castillo, F. Diaz, S. Vieweg. 2014. CrisisLex: A Lexicon for Collecting and Filtering Microblogged Communications in Crises. In Proceedings of the AAAI Conference on Weblogs and Social Media (ICWSM'14). AAAI Press, Ann Arbor, MI, USA.</li>\n",
    "        <li>Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz and Patrick Meier. Extracting Information Nuggets from Disaster-Related Messages in Social Media. In Proceedings of the 10th International Conference on Information Systems for Crisis Response and Management (ISCRAM), May 2013, Baden-Baden, Germany.</li>\n",
    "        <li>Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz and Patrick Meier. Practical Extraction of Disaster-Relevant Information from Social Media. In Social Web for Disaster Management (SWDM'13) - Co-located with WWW, May 2013, Rio de Janeiro, Brazil.</li>\n",
    "        <li>https://appen.com/datasets/combined- disaster-response-data/</li>\n",
    "        <li>https://data.world/crowdflower/disasters- on-social-media</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f0a9b",
   "metadata": {},
   "source": [
    "### Pull text into notebook and establish variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3f666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folders\n",
    "labled_data_folder  =  os.path.join(parent_dir,\"Data/crisis_datasets_benchmarks/all_data_en\")\n",
    "initial_filtering_folder = os.path.join(parent_dir,\"Data/crisis_datasets_benchmarks/initial_filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2bd69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_pull_folder = os.path.join(parent_dir,\"Data/scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "421f1b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>@ErinBrockovich We haven't had running water i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>@ErinBrockovich We haven't had running water i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>NHS College Storm still battling in Bemidji, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>I hope one of the last two challenges that hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>@atliberalandold Once after an ice storm in Ka...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TweetId                                               Text\n",
       "0  1.362915e+18  @ErinBrockovich We haven't had running water i...\n",
       "1  1.362915e+18  @ErinBrockovich We haven't had running water i...\n",
       "2  1.362915e+18  NHS College Storm still battling in Bemidji, t...\n",
       "3  1.362915e+18  I hope one of the last two challenges that hav...\n",
       "4  1.362915e+18  @atliberalandold Once after an ice storm in Ka..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotweets = pd.read_csv(os.path.join(self_pull_folder,\"tweetsid.csv\"))\n",
    "geotweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd835de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish dataframes\n",
    "# filtered  = pd.read_table(os.path.join\n",
    "#                        (initial_filtering_folder,\n",
    "#                                     \"crisis_consolidated_informativeness_filtered_lang.tsv\"))\n",
    "# english = filtered[filtered[\"lang\"] == 'en']\n",
    "\n",
    "train =  pd.read_table(os.path.join\n",
    "                       (labled_data_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang_en_train.tsv\"))\n",
    "test  =  pd.read_table(os.path.join\n",
    "                       (labled_data_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang_en_train.tsv\"))\n",
    "dev =  pd.read_table(os.path.join\n",
    "                     (labled_data_folder,\n",
    "                                  \"crisis_consolidated_informativeness_filtered_lang_en_train.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "199437a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [train, test, dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e196f",
   "metadata": {},
   "source": [
    "#### Learning how to do some text stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8898953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproccessed_tweets_train = process(train['text'])\n",
    "preproccessed_tweets_test = process(test['text'])\n",
    "preproccessed_tweets_dec = process(dev['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48512f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproccessed_tweets_geotweets = process(geotweets ['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8fb72b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test['processed2'] = preproccessed_tweets_train\n",
    "train['processed2'] = preproccessed_tweets_test\n",
    "dev['processed2'] = preproccessed_tweets_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b887a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotweets['processed2'] = preproccessed_tweets_geotweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d37af745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>lang_conf</th>\n",
       "      <th>class_label</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530</td>\n",
       "      <td>disaster_events</td>\n",
       "      <td>drd-figureeight-multimedia</td>\n",
       "      <td>Organization that are working in Haiti, I do n...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>informative</td>\n",
       "      <td>organization that are working in haiti i do no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>913070034204884992</td>\n",
       "      <td>hurricane_maria</td>\n",
       "      <td>crisismmd</td>\n",
       "      <td>Maria now a hurricane again!! Strong storm sur...</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>informative</td>\n",
       "      <td>maria now a hurricane again !! strong storm su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>540027128478044160</td>\n",
       "      <td>2014_philippines_typhoon</td>\n",
       "      <td>crisisnlp-cf</td>\n",
       "      <td>RT @ANCALERTS: Fallen tree branches scattered ...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.950529</td>\n",
       "      <td>informative</td>\n",
       "      <td>rt usrId fallen tree branches scattered in sor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17711</td>\n",
       "      <td>disaster_events</td>\n",
       "      <td>drd-figureeight-multimedia</td>\n",
       "      <td>The Government did not request international a...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>informative</td>\n",
       "      <td>the government did not request international h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778251007</td>\n",
       "      <td>disaster_events</td>\n",
       "      <td>dsm-cf</td>\n",
       "      <td>Remove http://t.co/77b2rNRTt7 Browser Hijacker...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>remove httpAddress browser hijacker how httpAd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                     event                      source  \\\n",
       "0                 530           disaster_events  drd-figureeight-multimedia   \n",
       "1  913070034204884992           hurricane_maria                   crisismmd   \n",
       "2  540027128478044160  2014_philippines_typhoon                crisisnlp-cf   \n",
       "3               17711           disaster_events  drd-figureeight-multimedia   \n",
       "4           778251007           disaster_events                      dsm-cf   \n",
       "\n",
       "                                                text lang  lang_conf  \\\n",
       "0  Organization that are working in Haiti, I do n...   en   1.000000   \n",
       "1  Maria now a hurricane again!! Strong storm sur...   en        NaN   \n",
       "2  RT @ANCALERTS: Fallen tree branches scattered ...   en   0.950529   \n",
       "3  The Government did not request international a...   en   1.000000   \n",
       "4  Remove http://t.co/77b2rNRTt7 Browser Hijacker...   en   1.000000   \n",
       "\n",
       "       class_label                                          processed  \n",
       "0      informative  organization that are working in haiti i do no...  \n",
       "1      informative  maria now a hurricane again !! strong storm su...  \n",
       "2      informative  rt usrId fallen tree branches scattered in sor...  \n",
       "3      informative  the government did not request international h...  \n",
       "4  not_informative  remove httpAddress browser hijacker how httpAd...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43a168ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetId</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>@ErinBrockovich We haven't had running water i...</td>\n",
       "      <td>we haven t had running water in lake charles l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>@ErinBrockovich We haven't had running water i...</td>\n",
       "      <td>we haven t had running water in lake charles l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>NHS College Storm still battling in Bemidji, t...</td>\n",
       "      <td>nhs college storm still battling in bemidji tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>I hope one of the last two challenges that hav...</td>\n",
       "      <td>i hope one of the last two challenges that hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.362915e+18</td>\n",
       "      <td>@atliberalandold Once after an ice storm in Ka...</td>\n",
       "      <td>atliberalandold once after an ice storm in kan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TweetId                                               Text  \\\n",
       "0  1.362915e+18  @ErinBrockovich We haven't had running water i...   \n",
       "1  1.362915e+18  @ErinBrockovich We haven't had running water i...   \n",
       "2  1.362915e+18  NHS College Storm still battling in Bemidji, t...   \n",
       "3  1.362915e+18  I hope one of the last two challenges that hav...   \n",
       "4  1.362915e+18  @atliberalandold Once after an ice storm in Ka...   \n",
       "\n",
       "                                          processed2  \n",
       "0  we haven t had running water in lake charles l...  \n",
       "1  we haven t had running water in lake charles l...  \n",
       "2  nhs college storm still battling in bemidji tr...  \n",
       "3  i hope one of the last two challenges that hav...  \n",
       "4  atliberalandold once after an ice storm in kan...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0154c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  85.6%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "train_counts = cv.fit_transform(train['processed'])\n",
    "test_data = cv.transform(test['processed'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "clf = nb.fit(train_counts, train['class_label'])\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01d67fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "train_counts = cv.fit_transform(train['processed'])\n",
    "test_data = cv.transform(geotweets['processed2'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "clf = nb.fit(train_counts, train['class_label'])\n",
    "predicted_geo = clf.predict(test_data)\n",
    "\n",
    "\n",
    "# print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "990c31a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Federal Energy Regulatory Commission urged Texas to winterize its power plants with insulation and heat pipes in 2011.  The state deliberately cut its power grid off from the rest of the country precisely to escape federal regulation and did not act. https://t.co/J36cPXcT5j\n",
      "the federal energy regulatory commission urged texas to winterize its power plants with insulation and heat pipes in the state deliberately cut its power grid off from the rest of the country precisely to escape federal regulation and did not\n",
      "informative\n"
     ]
    }
   ],
   "source": [
    "geotweets['predict']=predicted_geo\n",
    "print(geotweets.iloc[244]['Text'] )\n",
    "print (geotweets.iloc[244]['processed2'])\n",
    "print (geotweets.iloc[244]['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386b3048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  85.6%\n"
     ]
    }
   ],
   "source": [
    "# Read about Pipelines here:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "clf = clf.fit(train['processed'], train['class_label'])\n",
    "predicted = clf.predict(test['processed'])\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed'], test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "004cfca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with Stop Words) prediction accuracy =  84.9%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tools = [('tf', TfidfVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "# set_params() of TfidfVectorizer below, sets the parameters of the estimator. The method works on simple estimators as \n",
    "# well as on nested objects (such as pipelines). The pipelines have parameters of the form <component>__<parameter> \n",
    "# so that it’s possible to update each component of a nested object.\n",
    "clf.set_params(tf__stop_words = 'english')\n",
    "\n",
    "clf = clf.fit(train['processed'], train['class_label'])\n",
    "predicted = clf.predict(test['processed'])\n",
    "\n",
    "print(\"NB (TF-IDF with Stop Words) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed'], test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "399f8790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  86.2%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "train_counts = cv.fit_transform(train['processed2'])\n",
    "test_data = cv.transform(test['processed2'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "clf = nb.fit(train_counts, train['class_label'])\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7bd039e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  86.2%\n"
     ]
    }
   ],
   "source": [
    "# Read about Pipelines here:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "clf = clf.fit(train['processed2'], train['class_label'])\n",
    "predicted = clf.predict(test['processed2'])\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed2'], test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b875ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB (TF-IDF with Stop Words) prediction accuracy =  85.5%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tools = [('tf', TfidfVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "# set_params() of TfidfVectorizer below, sets the parameters of the estimator. The method works on simple estimators as \n",
    "# well as on nested objects (such as pipelines). The pipelines have parameters of the form <component>__<parameter> \n",
    "# so that it’s possible to update each component of a nested object.\n",
    "clf.set_params(tf__stop_words = 'english')\n",
    "\n",
    "clf = clf.fit(train['processed2'], train['class_label'])\n",
    "predicted = clf.predict(test['processed2'])\n",
    "\n",
    "print(\"NB (TF-IDF with Stop Words) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed2'], test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "246e3531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Elements/DataScience/anaconda3/envs/machinelearning/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR (TF-IDF with Stop Words) prediction accuracy =  88.5%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(stop_words = 'english')),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('lr', LogisticRegression())])\n",
    "\n",
    "\n",
    "clf = clf.fit(train['processed2'], train['class_label'])\n",
    "predicted = clf.predict(test['processed2'])\n",
    "predicted_geo_lab = clf.predict(geotweets['processed2'])\n",
    "geotweets['predict']=predicted_geo_lab\n",
    "\n",
    "print(\"LR (TF-IDF with Stop Words) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed2'], test['class_label'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73304871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2059, 1990, 1701, 1589, 1009, 1773, 1219, 297, 1209, 1324]\n",
      "Predicted Label informative\n",
      "initial tweet WATCH: These camels watched as snow fell in the mountains near Tabuk, Saudi Arabia on Thursday.\n",
      "https://t.co/mvRdOJZCZm https://t.co/IkvQox3D4L\n",
      "Processed tweet watch these camels watched as snow fell in the mountains near tabuk saudi arabia on\n",
      "Predicted Label not_informative\n",
      "initial tweet Icepocalypse, Austin, TX.. a pictorial:\n",
      "1. Terracotta worked minimally, better if setup as a true radiator in a small room.\n",
      "2. Non-potable melted snow/ice for dishes, rinsing, flushing.\n",
      "3. Sanitizer in the bathroom because of the boil notice.\n",
      "4. OMG oatmeal was a life saver! https://t.co/Xrfe7FCLkp\n",
      "Processed tweet icepocalypse austin a terracotta worked minimally better if setup as a true radiator in a small non potable melted for dishes rinsing sanitizer in the bathroom because of the boil omg oatmeal was a life saver !\n",
      "Predicted Label not_informative\n",
      "initial tweet @MileHigh_Nick @MileHighChubb @JacobThaGoat @Goaty_Szn It’s a change of power now. Look at what the Vikings did with safeties in Minnesota. Expect the same in Denver. With a defensive minded coach who loves good safeties.\n",
      "Processed tweet milehigh nick milehighchubb jacobthagoat goaty szn it’s a change of power look at what the vikings did with safeties in expect the same in with a defensive minded coach who loves good\n",
      "Predicted Label informative\n",
      "initial tweet @USA_Polling Also, the map of power outage aligns pretty well with the map of electric grids. Texas is collapsing because we rely on our own grid and it’s overwhelmed. https://t.co/KO2Qx7NwP1\n",
      "Processed tweet usa polling also the map of power outage aligns pretty well with the map of electric texas is collapsing because we rely on our own grid and it’s\n",
      "Predicted Label informative\n",
      "initial tweet @RancidGinger Is this guy the Texas senator who jetted off to holiday when there was snow storms ? Can Texas get snow storms its too far south right ?\n",
      "Processed tweet is this guy the texas senator who jetted off to holiday when there was snow storms ? can texas get snow storms its too far south right ?\n",
      "Predicted Label informative\n",
      "initial tweet San Antonio, TX:\n",
      "3-1-1 is available to help get food/water to elderly and disabled individuals. https://t.co/KiaX5dFkG0\n",
      "Processed tweet san antonio tx 3 1 1 is available to help get to elderly and disabled\n",
      "Predicted Label not_informative\n",
      "initial tweet @dr9ws you eating ice sandwhiches out there in texas rn shuddup\n",
      "Processed tweet you eating ice sandwhiches out there in texas rn shuddup\n",
      "Predicted Label not_informative\n",
      "initial tweet @GFizeek @mlbandy @_dennis_system Absolutely the experience was very different - I thought we were debating from a purely musical standpoint\n",
      "Processed tweet absolutely the experience was very different i thought we were debating from a purely musical standpoint\n",
      "Predicted Label not_informative\n",
      "initial tweet Notre Dame vs Darien | Today's Connecticut High School Ice Hockey Live Stream\n",
      "To Watch Here : https://t.co/4q7OH1FejQ\n",
      "The Darien (CT) varsity ice hockey team has a home non-conference game vs. Notre Dame (West Haven, CT) today @\n",
      "Processed tweet notre dame vs darien today s connecticut high school ice hockey live stream to watch here darien ct varsity ice hockey team has a home non conference game vs . notre dame west haven ct today\n",
      "Predicted Label informative\n",
      "initial tweet starting another fire early since now our power is off @ Austin, Texas https://t.co/xGP9O8pGSO\n",
      "Processed tweet starting another fire early since now our power is off austin texas\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#Generate 10 random numbers between 0 and 30\n",
    "randomlist = random.sample(range(0, len(geotweets)), 10)\n",
    "print(randomlist)\n",
    "for z in randomlist:\n",
    "    print(\"Predicted Label {0}\".format(geotweets.iloc[z]['predict']))\n",
    "    print(\"initial tweet {0}\".format(geotweets.iloc[z]['Text']))\n",
    "    print(\"Processed tweet {0}\".format(geotweets.iloc[z]['processed2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3f1a376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " informative       0.95      0.98      0.96     65612\n",
      "         not       0.96      0.92      0.94     43829\n",
      "\n",
      "    accuracy                           0.95    109441\n",
      "   macro avg       0.96      0.95      0.95    109441\n",
      "weighted avg       0.95      0.95      0.95    109441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase and restrict ourselves to about half the available features\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(train['processed2'], train['class_label'])\n",
    "y_pred = pclf.predict(test['processed2'])\n",
    "print(metrics.classification_report(test['class_label'], y_pred, target_names = ['informative','not']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cb64758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale, LabelBinarizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random seed for numpy\n",
    "np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c77f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Build a mode that is composed of this list of layers\n",
    "model = Sequential(\n",
    "    [\n",
    "          # This specifies a single neuron, and the input is 2 numbers.\n",
    "    Dense(1, input_dim=2),  # a dense layer, every neuron is connected to all points from the lower layer (input)\n",
    "    Activation('linear'),   # Specify the type of decision surface, i.e., simple linear regression\n",
    "    Dense(1),               # another dense layer, input_dim is inferred from the previous layer's output\n",
    "    Activation('sigmoid')   # Specify the type of decision surface, i.e., simple logistic regression\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44837b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 9\n",
    "message = english.iloc[n]['text']\n",
    "message_procc =  preproccessed_tweets [n]\n",
    "print(message)\n",
    "print(message_procc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00982481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate counts of tokens, using string functionality\n",
    "\n",
    "\n",
    "# Used to print sequences in a nice manner\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80,compact=True)\n",
    "\n",
    "# Tokenize the message and create a counter for frequency of each word in message.\n",
    "# Browse for split() python or go to this link http://www.pythonforbeginners.com/dictionary/python-split to see what split() does\n",
    "words = message.split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate counts of tokens, using string functionality\n",
    "\n",
    "\n",
    "# Used to print sequences in a nice manner\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80,compact=True)\n",
    "\n",
    "# Tokenize the message and create a counter for frequency of each word in message.\n",
    "# Browse for split() python or go to this link http://www.pythonforbeginners.com/dictionary/python-split to see what split() does\n",
    "words = message_procc.split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = message.lower().split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twks= twokenize.tokenizeRawTweetText(message)\n",
    "word_count = col.Counter(twks)\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below print statememt, {0:12s} means, print argument 1 with 12 spaces allocated for it. \n",
    "# You can see that for two of the results, 12 spaces is not adequate and these results are misaligned.\n",
    "print('{0:12s}: {1}'.format('Term', 'Frequency'))\n",
    "print(20*'-')\n",
    "\n",
    "total_word_count = sum(word_count.values())\n",
    "for count in word_count.most_common(counts_to_display):\n",
    "    pp.pprint('{0:12s}: {1:4.3f}'.format(count[0], count[1]/total_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8efc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', lowercase=True)\\\n",
    "cv.fit(english['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now process documents.\n",
    "\n",
    "# We need an iteratable to apply cv.transform()\n",
    "msg = []\n",
    "msg.append(message)\n",
    "\n",
    "# Transforming a single message is easier to comprehend. By default, scikit learn uses sparse matrices for text processing\n",
    "# It returns a Document Term Matrix (dtm)\n",
    "dtm = cv.transform(msg)\n",
    "\n",
    "# In sparse format number of tokens indicate size of dataset vocabulary. \n",
    "# So there is 1 document and 130107 featues in the dtm.\n",
    "print('Number of Samples = {0}'.format(dtm.shape[0]))\n",
    "print('Number of Tokens = {0}'.format(dtm.shape[1]))\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "# You can't explore the document-term matrix when it is in sparse form. We can convert from sparse to dense form to explore \n",
    "# the document-term matrix. The range given below is chosen randomly. \n",
    "# Each word is a feature. Below zeros indicate the words/features in columns 1000 to 1100, those words do not appear in \n",
    "# the input message. Thats why we have zeros for those cells\n",
    "print(dtm.todense()[:,1000:1100])\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "# We can also print only nonzero DTM matrix elements. \n",
    "print('Cells from Document-Term Matrix[i, j] and c (Count)')\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "\n",
    "# Find non-zero elements. scipy.sparse.find() returns the indices and values of the nonzero elements of a matrix.\n",
    "# i,j contains the row and column indices where non zero matrix entries are present while V has the entry's value.\n",
    "i, j, V = sp.find(dtm)\n",
    "dtm_list = list(zip(i, j, V))\n",
    "pp.pprint(dtm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bad798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.vocabulary_[\"monsoon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtm_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dtm_list, key=itemgetter(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the terms in the vocabulary\n",
    "terms = cv.vocabulary_\n",
    "\n",
    "# Look for a single term confuse\n",
    "search_word = 'blizzard'\n",
    "print(\"Chosen Word ({0}): Column = {1}\".format(search_word, terms[search_word]))\n",
    "\n",
    "# Find the maximum value in dtm_list in 3rd column which will be 114455\n",
    "max_key = max(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "# Find the minimum value in dtm_list in 3rd column which will be 2336\n",
    "min_key = min(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "# In the below two lines, terms.keys() will return all keys - i.e. the column names(words).\n",
    "# For loop iterates over all this words to see get the column name which matches the column index we have in max_key and min_key\n",
    "x_max = [key for key in terms.keys() if terms[key] == max_key]\n",
    "x_min = [key for key in terms.keys() if terms[key] == min_key]\n",
    "\n",
    "# the for loop above returned a list as output. So x_max is a list with a column name as same with x_min\n",
    "print(\"Max Word ({0}): Column = {1}\".format(x_max[0], max_key))\n",
    "print(\"Min Word ({0}): Column = {1}\".format(x_min[0], min_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd50d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize a text document\n",
    "# word_tokenize() is tokenizing the message and each word is being converted to lowercase. \n",
    "# So words has the vocabulary of message\n",
    "words = [word.lower() for word in nltk.word_tokenize(message)]\n",
    "top_display=25\n",
    "\n",
    "# Count number of occurances for each token\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd4d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a text document\n",
    "# word_tokenize() is tokenizing the message and each word is being converted to lowercase. \n",
    "# So words has the vocabulary of message\n",
    "words = [word.lower() for word in nltk.word_tokenize(message_procc)]\n",
    "top_display=25\n",
    "\n",
    "# Count number of occurances for each token\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Regular Expression to parse a text document\n",
    "\n",
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', message_procc))]\n",
    "\n",
    "# Count token occurances\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Message has %i tokens and %i words for a lexical diversity of %0.3f\" % (num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of unique tokens (or bins)\n",
    "print('Number of unique bins(tokens) = {0}\\n'.format(counts.B()))\n",
    "print('Number of sample outcomes = {0}\\n'.format(counts.N()))\n",
    "print('Maximum occuring token = {0}\\n'.format(counts.max()))\n",
    "\n",
    "print('{0:12s}: {1}'.format('Term', 'Count'))\n",
    "print(25*'-')\n",
    "\n",
    "for token, freq in counts.most_common(top_display):\n",
    "    print('{0:12s}:  {1:4.3f}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapaxes\n",
    "pp.pprint(counts.hapaxes()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of elements to display\n",
    "top_display=10\n",
    "counts.tabulate(top_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5befe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "sns.set(style=\"white\", font_scale=1.5)\n",
    "sns.despine(offset=5)#, trim=True)\n",
    "counts.plot(top_display, cumulative=True)\n",
    "axs.set_title('Term Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498774b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentance to tokenize\n",
    "my_text = message_procc\n",
    "\n",
    "cv1 = CountVectorizer(lowercase=True)\n",
    "cv2 = CountVectorizer(stop_words = 'english', lowercase=True)\n",
    "\n",
    "tk_func1 = cv1.build_analyzer()\n",
    "tk_func2 = cv2.build_analyzer()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "print('Tokenization:')\n",
    "pp.pprint(tk_func1(my_text))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Tokenization (with Stop words):')\n",
    "pp.pprint(tk_func2(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_func1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46230f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_text = message_procc\n",
    "stemmer = PorterStemmer()\n",
    "tokens = nltk.word_tokenize(new_text)\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "for w in tokens:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8995d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "english['text']0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
