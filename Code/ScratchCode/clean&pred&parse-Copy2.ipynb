{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7cdda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "since_date = '2021-07-12'\n",
    "until_date = '2021-07-13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cd4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "\n",
    "                                       # to handle regular expressions\n",
    "              # to extract the puntuation symbols\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize          # to divide strings into tokens\n",
    "\n",
    "import tensorflow as tf            \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import text_proccessing\n",
    "from text_proccessing import clean_text\n",
    "from text_proccessing import remove_stopwords\n",
    "from text_proccessing import lemmatize_text\n",
    "from text_proccessing import concatenate_text\n",
    "from text_proccessing import makeglove\n",
    "from text_proccessing import make_embedding_matrix\n",
    "\n",
    "import modhelp\n",
    "from modhelp import train_val_split\n",
    "from modhelp import geo_df\n",
    "from modhelp import suggest_nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08db58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = os.getcwd()\n",
    "\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "data_dir = os.path.join(parent_dir,\"Data\")\n",
    "dsa= os.path.dirname(parent_dir)\n",
    "tweet_dir = os.path.join(parent_dir,\"TweetMap\")\n",
    "directory = os.path.join(data_dir,'splits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(directory, 'InformativenessTrain_Tokenized.csv'),engine='python')\n",
    "train_data['text']= train_data['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84b3bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitthis started at: 2021-07-16 18:53:57.186778\n",
      "Total size of the dataset: 117339.\n",
      "Training dataset: 88005.\n",
      "Validation dataset: 29334.\n",
      "Text done 0:00:00.022361\n",
      "Loading model,embedding and glove commenced at  2021-07-16 18:53:57.209476\n",
      "loading vecotor\n",
      "indexing\n",
      "matrix and vectorize\n",
      "Converted 32240 words (22760 misses).\n",
      "embedded in 0:02:29.803524\n"
     ]
    }
   ],
   "source": [
    "start =  datetime.datetime.now() \n",
    "print('splitthis started at: {}'.format(start))\n",
    "train_samples, val_samples, train_labels, val_labels = train_val_split(train_data, 0.25)\n",
    "end =  datetime.datetime.now() \n",
    "print ('Text done {}'.format(end-start))\n",
    "start =  datetime.datetime.now() \n",
    "print('Loading model,embedding and glove commenced at  {}'.format(start))\n",
    "\n",
    "model = tf.keras.models.load_model(os.path.join(dsa,'model1'))\n",
    "print(\"loading vecotor\")\n",
    "path_to_glove_file = os.path.join(dsa,'WordVector','glove.twitter.27B.200d.txt')\n",
    "\n",
    "print(\"indexing\")\n",
    "embeddings_index=makeglove(path_to_glove_file)\n",
    "print(\"matrix and vectorize\")\n",
    "embedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)\n",
    "end =  datetime.datetime.now() \n",
    "\n",
    "print(\"embedded in {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2de9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "twts = pd.read_csv(os.path.join(tweet_dir,'tweets_df_{0}_{1}.csv'.\n",
    "                              format(since_date.replace('-',''),until_date.replace('-',''))), engine = 'python')\n",
    "twts['Text'] = twts['Text'].astype(str)\n",
    "twts = twts[['Datetime', 'TweetId', 'Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d8e1f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-16 19:29:50.225716: processing \n",
      "2021-07-16 19:32:27.853390: processed in 0:02:37.627674\n"
     ]
    }
   ],
   "source": [
    "procstart =  datetime.datetime.now()\n",
    "print(\"{}: processing \".format(procstart))\n",
    "twts['ptext'] = twts['Text'].apply(lambda x: clean_text(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x: word_tokenize(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : remove_stopwords(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : lemmatize_text(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : concatenate_text(x))\n",
    "procend=  datetime.datetime.now() \n",
    "print(\"{}: processed in {}\".format(procend, procend - procstart))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8933ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-16 19:32:27.901063: predict\n",
      "predicted 0:00:47.802458\n",
      "working over 40582 informative tweets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predstart = datetime.datetime.now() \n",
    "print(\"{}: predict\".format(predstart))\n",
    "predictions = suggest_nn2(twts, model,vectorizer)\n",
    "\n",
    "submission_data = {\"ID\": twts['TweetId'].tolist(),\"tweet\": twts['Text'].tolist(), \"target\": predictions}\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "result = twts.join(submission_df.target)\n",
    "predend = datetime.datetime.now() \n",
    "print(\"predicted {}\".format(predend-predstart))\n",
    "# \n",
    "result_inf = result[result['target']==0]\n",
    "print(\"working over {} informative tweets\".format(len(result_inf)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49897543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Run geoprocessing over tweets\")\n",
    "\n",
    "geostart =  datetime.datetime.now() \n",
    "print(\"{}: geostart\".format(geostart))\n",
    "df_js = geo_df(result_inf)\n",
    "geoend =  datetime.datetime.now()\n",
    "print(\"{}: geoend {}\".format(geoend,geoend-geostart))\n",
    "\n",
    "df_js.to_csv(os.path.join(tweet_dir,'result_{0}_{1}.csv'.\n",
    "                              format(since_date.replace('-',''),until_date.replace('-',''))))\n",
    "print(\"f√≠n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65845974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_js\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636b049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
