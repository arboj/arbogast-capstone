{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cd4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import capstone_twitter_search\n",
    "from capstone_twitter_search import twittsearch\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize          # to divide strings into tokens\n",
    "\n",
    "import tensorflow as tf            \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import text_proccessing\n",
    "from text_proccessing import clean_text\n",
    "from text_proccessing import remove_stopwords\n",
    "from text_proccessing import lemmatize_text\n",
    "from text_proccessing import concatenate_text\n",
    "from text_proccessing import makeglove\n",
    "from text_proccessing import make_embedding_matrix\n",
    "from text_proccessing import processme\n",
    "\n",
    "import modhelp\n",
    "from modhelp import train_val_split\n",
    "from modhelp import geo_df\n",
    "from modhelp import suggest_nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08db58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "data_dir = os.path.join(parent_dir,\"Data\")\n",
    "directory = os.path.join(data_dir,'splits')\n",
    "dsa= os.path.dirname(parent_dir)\n",
    "tweet_dir = os.path.join(parent_dir,\"TweetMap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6cf8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(directory, 'InformativenessTrain_Tokenized.csv'),engine='python')\n",
    "train_data['text']= train_data['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e48490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitthis started at: 2021-07-16 19:39:20.432934\n",
      "Total size of the dataset: 117339.\n",
      "Training dataset: 88005.\n",
      "Validation dataset: 29334.\n",
      "Text done 0:00:00.813266\n",
      "Loading model,embedding and glove commenced at  2021-07-16 19:39:21.247367\n",
      "loading vecotor\n",
      "indexing\n",
      "matrix and vectorize\n",
      "Converted 32255 words (22745 misses).\n",
      "embedded in 0:03:03.237175\n"
     ]
    }
   ],
   "source": [
    "start =  datetime.datetime.now() \n",
    "print('splitthis started at: {}'.format(start))\n",
    "train_samples, val_samples, train_labels, val_labels = train_val_split(train_data, 0.25)\n",
    "end =  datetime.datetime.now() \n",
    "print ('Text done {}'.format(end-start))\n",
    "start =  datetime.datetime.now() \n",
    "print('Loading model,embedding and glove commenced at  {}'.format(start))\n",
    "\n",
    "model = tf.keras.models.load_model(os.path.join(dsa,'model1'))\n",
    "print(\"loading vecotor\")\n",
    "path_to_glove_file = os.path.join(dsa,'WordVector','glove.twitter.27B.200d.txt')\n",
    "\n",
    "print(\"indexing\")\n",
    "embeddings_index=makeglove(path_to_glove_file)\n",
    "print(\"matrix and vectorize\")\n",
    "embedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)\n",
    "end =  datetime.datetime.now() \n",
    "\n",
    "print(\"embedded in {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebeff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping commenced at 2021-07-16 19:42:24.927057\n",
      "(\"forest fire\") OR wildfire OR bushfire OR (extreme heat) OR (record heat) OR heatwave OR (\"heat wave\") OR typhoon OR cyclone OR hurricane OR tornado OR (\"storm surge\") OR  blizzard OR snow OR (\"ice storm\") OR sleet OR thunderstorm OR hail OR flood OR flooding OR freeze OR frost OR (extreme cold) OR landslide OR tsunami OR (\"tidal wave\") OR earthquake OR eruption OR volcano OR lava OR lahar OR avalanche OR mudslide OR sinkhole since:2021-07-11 until:2021-07-12 filter:has_engagement\n"
     ]
    }
   ],
   "source": [
    "scrapestart =  datetime.datetime.now() \n",
    "print(\"Scraping commenced at {}\".format(scrapestart))\n",
    "# =============================================================================\n",
    "text_query = '(\"forest fire\") OR wildfire OR bushfire OR \\\n",
    "(extreme heat) OR (record heat) OR heatwave OR (\"heat wave\") OR typhoon OR cyclone OR hurricane OR \\\n",
    "tornado OR (\"storm surge\") OR  blizzard OR snow OR (\"ice storm\") OR sleet OR thunderstorm OR \\\n",
    "hail OR flood OR flooding OR freeze OR frost OR (extreme cold) OR landslide OR tsunami OR (\"tidal wave\") OR \\\n",
    "earthquake OR eruption OR volcano OR lava OR lahar OR avalanche OR mudslide OR sinkhole'\n",
    "\n",
    "\n",
    "# qlist = [firemetq_geologicalotherq]\n",
    "# qname= ['fire & meteorlogical & geological & other']\n",
    "since_date = '2021-07-11'\n",
    "until_date = '2021-07-12'\n",
    "tweetcount = 150000\n",
    "twts = twittsearch(text_query, since_date,until_date,tweetcount)\n",
    "#  twittsearch(text_query,since_date,until_date,tweetcount).to_csv(os.path.join(tweet_dir,'tweets_df_{0}_{1}.csv'.\n",
    "#                               format(since_date.replace('-',''),until_date.replace('-',''))),\n",
    "#                  index =False)\n",
    "\n",
    "scrapend =  datetime.datetime.now()\n",
    "print(\"Scraping ended at {}\".format(scrapend))\n",
    "print(\"Scraping time {}\".format(scrapend-scrapestart))\n",
    "\n",
    "twts = processme(twts)\n",
    "scrapallend =  datetime.datetime.now()\n",
    "print(\"{}: Scraping time {}\".format(scrapallend, scrapallend-scrapestart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f61684",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predstart = datetime.datetime.now() \n",
    "print(\"{}: predict\".format(predstart))\n",
    "predictions = suggest_nn2(twts, model,vectorizer)\n",
    "\n",
    "submission_data = {\"ID\": twts['TweetId'].tolist(),\"tweet\": twts['Text'].tolist(), \"target\": predictions}\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "result = twts.join(submission_df.target)\n",
    "predend = datetime.datetime.now() \n",
    "print(\"predicted {}\".format(predend-predstart))\n",
    "# \n",
    "result_inf = result[result['target']==0]\n",
    "print(\"working over {} informative tweets\".format(len(result_inf)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49897543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Run geoprocessing over tweets\")\n",
    "\n",
    "geostart =  datetime.datetime.now() \n",
    "print(\"{}: geostart\".format(geostart))\n",
    "df_js = geo_df(result_inf)\n",
    "geoend =  datetime.datetime.now()\n",
    "print(\"{}: geoend {}\".format(geoend,geoend-geostart))\n",
    "\n",
    "df_js.to_csv(os.path.join(tweet_dir,'result_{0}_{1}.csv'.\n",
    "                              format(since_date.replace('-',''),until_date.replace('-',''))))\n",
    "print(\"f√≠n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c4b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
