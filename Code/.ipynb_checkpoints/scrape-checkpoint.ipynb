{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259ad2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f809a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "data_dir = os.path.join(parent_dir,\"Data\")\n",
    "tweet_dir = os.path.join(parent_dir,\"TweetMap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418fcb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar  1 10:40:16 2021\n",
    "\n",
    "@author: Arbo\n",
    "\"\"\"\n",
    "\n",
    "def twittsearch(text_query,since_date,until_date,tweetcount):\n",
    "    tweets_list = []\n",
    "\n",
    "    query = '{0} since:{1} until:{2} filter:has_engagement'.format(text_query, since_date, until_date)\n",
    "    print(query)\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "\n",
    "        if i>tweetcount:\n",
    "\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.content, \n",
    "                            tweet.lang,tweet.retweetedTweet,tweet.quotedTweet])\n",
    "\n",
    "    # Creating a dataframe from the tweets list above\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, \n",
    "                             columns=['Datetime', 'TweetId', 'Text',\n",
    "                                     \"Language\", \"RTFrom\", \"QTFrom\"])\n",
    "    \n",
    "    print(\"found {} tweets ranging from {} to {}\".format(len(tweets_df),\n",
    "                                                            tweets_df.Datetime.min(),tweets_df.Datetime.max()))\n",
    "    \n",
    "    print(\"dropping duplicates\")   \n",
    "    tweets_df = tweets_df.drop_duplicates(subset=['TweetId'])\n",
    "    print(\"total of tweets now: {}\".format(len(tweets_df)))\n",
    "    print(\"english only\")\n",
    "\n",
    "    tweets_df = tweets_df[tweets_df[\"Language\"]=='en']\n",
    "    print(\"total of tweets now: {}\".format(len(tweets_df)))\n",
    "    tweets_df = tweets_df[tweets_df[\"RTFrom\"].isna()]\n",
    "    print(\"total of tweets now: {}\".format(len(tweets_df)))\n",
    "    tweets_df = tweets_df[tweets_df[\"QTFrom\"].isna()]\n",
    "    print(\"total of tweets now: {} ranging from {} to {}\".format(len(tweets_df),\n",
    "                                                                tweets_df.Datetime.min(),tweets_df.Datetime.max()))\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707815a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping commenced at 2021-07-14 22:34:58.689469\n",
      "(\"forest fire\") OR wildfire OR bushfire OR (extreme heat) OR (record heat) OR heatwave OR (\"heat wave\") OR typhoon OR cyclone OR hurricane OR tornado OR (\"storm surge\") OR  blizzard OR snow OR (\"ice storm\") OR sleet OR thunderstorm OR hail OR flood OR flooding OR freeze OR frost OR (extreme cold) OR landslide OR tsunami OR (\"tidal wave\") OR earthquake OR eruption OR volcano OR lava OR lahar OR avalanche OR mudslide OR sinkhole since:2021-07-12 until:2021-07-13 filter:has_engagement\n",
      "found 129482 tweets ranging from 2021-07-12 00:00:00+00:00 to 2021-07-12 23:59:58+00:00\n",
      "dropping duplicates\n",
      "total of tweets now: 129482\n",
      "english only\n",
      "total of tweets now: 64246\n",
      "total of tweets now: 64246\n",
      "total of tweets now: 59140 ranging from 2021-07-12 00:00:00+00:00 to 2021-07-12 23:59:57+00:00\n",
      "Scraping ended at 2021-07-14 22:56:15.198090\n",
      "Scraping time 0:21:16.508621\n"
     ]
    }
   ],
   "source": [
    "scrapestart =  datetime.datetime.now() \n",
    "print(\"Scraping commenced at {}\".format(scrapestart))\n",
    "# =============================================================================\n",
    "text_query = '(\"forest fire\") OR wildfire OR bushfire OR \\\n",
    "(extreme heat) OR (record heat) OR heatwave OR (\"heat wave\") OR typhoon OR cyclone OR hurricane OR \\\n",
    "tornado OR (\"storm surge\") OR  blizzard OR snow OR (\"ice storm\") OR sleet OR thunderstorm OR \\\n",
    "hail OR flood OR flooding OR freeze OR frost OR (extreme cold) OR landslide OR tsunami OR (\"tidal wave\") OR \\\n",
    "earthquake OR eruption OR volcano OR lava OR lahar OR avalanche OR mudslide OR sinkhole'\n",
    "\n",
    "\n",
    "# qlist = [firemetq_geologicalotherq]\n",
    "# qname= ['fire & meteorlogical & geological & other']\n",
    "since_date = '2021-07-12'\n",
    "until_date = '2021-07-13'\n",
    "tweetcount = 150000\n",
    "# tweets_df = twittsearch(qname,qlist,since_date,until_date,tweetcount)\n",
    "twittsearch(text_query,since_date,until_date,tweetcount).to_csv(os.path.join(tweet_dir,'tweets_df_{0}_{1}.csv'.\n",
    "                              format(since_date.replace('-',''),until_date.replace('-',''))),\n",
    "                 index =False)\n",
    "# =============================================================================\n",
    "scrapend =  datetime.datetime.now()\n",
    "print(\"Scraping ended at {}\".format(scrapend))\n",
    "print(\"Scraping time {}\".format(scrapend-scrapestart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3b16a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Elements/DataScience/dsa/capstone/TweetMap/tweets_df_20210712_20210713.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(tweet_dir,'tweets_df_{0}_{1}.csv'.\n",
    "                              format(since_date.replace('-',''),until_date.replace('-','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(os.path.join(tweet_dir,'tweets_df_{0}_{1}.csv'.\n",
    "                              format(since_date.replace('-',''),until_date.replace('-',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tweets_df[tweets_df.QTFrom.isna() ]\n",
    "t2.QTFrom.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b32ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df.to_csv(os.path.join(tweet_dir,'tweets_df_{0}_{1}.csv'.\n",
    "#                               format(since_date.replace('-',''),until_date.replace('-',''))),\n",
    "#                  index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e38f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d3d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
