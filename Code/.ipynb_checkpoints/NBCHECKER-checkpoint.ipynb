{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e17695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started at: 2021-08-11 17:35:27.041628\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'text_proccessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f025f2726804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtext_proccessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtext_proccessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtext_proccessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'text_proccessing'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Aug 10 23:46:58 2021\n",
    "\n",
    "@author: Arbo\n",
    "\"\"\"\n",
    "import datetime\n",
    "overallstart = datetime.datetime.now() \n",
    "print (\"started at: {}\".format(overallstart))\n",
    "\n",
    "import numpy as np                               # linear algebra\n",
    "import pandas as pd                              # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re                                        # to handle regular expressions\n",
    "from string import punctuation                   # to extract the puntuation symbols\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize          # to divide strings into tokens\n",
    "from nltk.stem import WordNetLemmatizer          # to lemmatize the tokens\n",
    "from nltk.corpus import stopwords                # to remove the stopwords \n",
    "\n",
    "import random                                    # for generating (pseudo-)random numbers\n",
    "import matplotlib.pyplot as plt                  # to plot some visualizations\n",
    "\n",
    "import tensorflow as tf            \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "# =============================================================================\n",
    "# from kerastuner.tuners import RandomSearch\n",
    "# from kerastuner.tuners import BayesianOptimization\n",
    "# from kerastuner.tuners import Hyperband\n",
    "# import kerastuner as kt\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "import text_proccessing\n",
    "from text_proccessing import clean_text\n",
    "from text_proccessing import remove_stopwords\n",
    "from text_proccessing import lemmatize_text\n",
    "from text_proccessing import concatenate_text\n",
    "from text_proccessing import makeglove\n",
    "from text_proccessing import make_embedding_matrix\n",
    "from modhelp import train_val_split\n",
    "from modhelp import test_listerine\n",
    "from modhelp import suggest_nn3\n",
    "from modhelp import initialize_nn\n",
    "from modhelp import  train_nn\n",
    "\n",
    "starti = datetime.datetime.now() \n",
    "print(\"{}: modules loaded in {} loading data\".format(starti, starti-overallstart))\n",
    "code_dir = os.getcwd()\n",
    "print(\"Current working directory: {0}\".format(code_dir))\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "data_dir = os.path.join(parent_dir,\"Data\")\n",
    "tweet_dir = os.path.join(parent_dir,\"TweetMap\")\n",
    "directory = os.path.join(data_dir,'splits')\n",
    "dsa= os.path.dirname(parent_dir)\n",
    "model = tf.keras.models.load_model(os.path.join(dsa,'model1'))\n",
    "train_data = pd.read_csv(os.path.join(directory, 'InformativenessTrain_Processed.csv'))\n",
    "test_data  = pd.read_csv(os.path.join(directory, 'InformativenessTest_Processed.csv'))\n",
    "dtl = datetime.datetime.now()\n",
    "print(\"{}: data loaded in {} cleaning text elapsed: {}\".format(starti,dtl-starti,dtl-overallstart))\n",
    "\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x: clean_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: clean_text(x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x:word_tokenize(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x:word_tokenize(x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x : remove_stopwords(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : remove_stopwords(x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x : lemmatize_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : lemmatize_text(x))\n",
    "train_data['text'] = train_data['text'].apply(lambda x : concatenate_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : concatenate_text(x))\n",
    "\n",
    "step1 = datetime.datetime.now()\n",
    "print(\"{}: text cleand in {} loading vector and spliting samples elapsed: {}\".format(step1,dtl-step1, step1-overallstart))\n",
    "path_to_glove_file = os.path.join(dsa,'WordVector','glove.twitter.27B.200d.txt')\n",
    "train_samples, val_samples, train_labels, val_labels = train_val_split(train_data, 0.25)\n",
    "# =============================================================================\n",
    "# test_samples, test_labels = test_listerine(test_data)\n",
    "# =============================================================================\n",
    "print(\"indexing\")\n",
    "embeddings_index=makeglove(path_to_glove_file)\n",
    "step2 = datetime.datetime.now()\n",
    "print(\"{}: indexed {} matrix and vectorize ellapsed {}\".format(step2, step2-step1, step2-overallstart))\n",
    "embedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)\n",
    "step3 = datetime.datetime.now()\n",
    "print(\"{}: mtx'd at vct'd in {} loading variables as vectors ellapsed {}\".format(step3, step3-step2,  step3-overallstart))\n",
    "\n",
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "# =============================================================================\n",
    "# x_test = vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
    "# =============================================================================\n",
    "y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "y_val = np.asarray(val_labels).astype('float32').reshape((-1,1))\n",
    "# =============================================================================\n",
    "# y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))\n",
    "# =============================================================================\n",
    "step4 = datetime.datetime.now()\n",
    "print(\"{}: training data ready in {} buildinf initial mod  ellapsed {}\".format(step4, step4-step3,  step4-overallstart))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = suggest_nn3(test_data, model,vectorizer)\n",
    "\n",
    "submission_data = {\"ID\": test_data['id'].tolist(),\"tweet\": test_data['text'].tolist(), \"target\": predictions}\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "result = test_data.join(submission_df.target)\n",
    "\n",
    "result.to_csv(os.path.join(tweet_dir,\"dubblecheck\"),index =False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed833e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
