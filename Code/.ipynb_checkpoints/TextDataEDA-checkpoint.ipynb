{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0543eb34",
   "metadata": {},
   "source": [
    "# Text Conditioning and Machine Learning\n",
    "\n",
    "This notebook lays out the process of some exploratory data analysis for the twitter disaster data pulled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1859dc",
   "metadata": {},
   "source": [
    "## Python Packages Used\n",
    "This notebook was set up in an environment running Python 3.8 with the following packages:\n",
    "pandas, tensorflow, keras, scikit-learn, nltk, gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b272dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import collections as col\n",
    "import pprint\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import twokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy.stats as stats\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dc5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a19dc9",
   "metadata": {},
   "source": [
    "## [Crisis Benchmark data for training Models](https://crisisnlp.qcri.org/crisis_datasets_benchmarks.html)\n",
    "\n",
    "<p>The crisis benchmark dataset consists data from several different data sources such as CrisisLex (<a href=\"http://crisislex.org/data-collections.html#CrisisLexT26\" target=\"_blank\">CrisisLex26</a>, <a href=\"http://crisislex.org/data-collections.html#CrisisLexT6\" target=\"_blank\">CrisisLex6</a>), <a href=\"https://crisisnlp.qcri.org/lrec2016/lrec2016.html\" target=\"_blank\">CrisisNLP</a>, <a href=\"http://mimran.me/papers/imran_shady_carlos_fernando_patrick_practical_2013.pdf\" target=\"_blank\">SWDM2013</a>, <a href=\"http://mimran.me/papers/imran_shady_carlos_fernando_patrick_iscram2013.pdf\" target=\"_blank\">ISCRAM13</a>, Disaster Response Data (DRD), <a href=\"https://data.world/crowdflower/disasters-on-social-media\" target=\"_blank\">Disasters on Social Media (DSM)</a>, <a href=\"https://crisisnlp.qcri.org/crisismmd\" target=\"_blank\">CrisisMMD</a> and data from <a href=\"http://aidr.qcri.org/\" target=\"_blank\">AIDR</a>. \n",
    "\t  The class label was mapped, remove duplicates removed and this was provided as a benchmark results for the community. </p>\n",
    "\n",
    "The authors have their model and data availible on github at <a href=\"https://github.com/firojalam/crisis_datasets_benchmarks\">https://github.com/firojalam/crisis_datasets_benchmarks</a>    </p>\n",
    "\n",
    "#### Data Availible from: https://crisisnlp.qcri.org/data/crisis_datasets_benchmarks/crisis_datasets_benchmarks_v1.0.tar.gz\n",
    "<h4><strong>References</strong></h4>\n",
    "<ol>\n",
    "<li><a href=\"http://sites.google.com/site/firojalam/\">Firoj Alam</a>, <a href=\"https://hsajjad.github.io/\">Hassan Sajjad</a>, <a href=\"http://mimran.me/\">Muhammad Imran</a> and <a href=\"https://sites.google.com/site/ferdaofli/\">Ferda Ofli</a>, <a href=\"https://arxiv.org/abs/2004.06774\" target=\"_blank\"><strong>CrisisBench: Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing,</strong></a> In ICWSM, 2021. [<a href=\"crisis_dataset_bib1.html\">Bibtex</a>]\n",
    "        </li>\n",
    "<!-- <li><a href=\"http://sites.google.com/site/firojalam/\">Firoj Alam</a>, <a href=\"https://hsajjad.github.io/\">Hassan Sajjad</a>, <a href=\"http://mimran.me/\">Muhammad Imran</a> and <a href=\"https://sites.google.com/site/ferdaofli/\">Ferda Ofli</a>, <a href=\"https://arxiv.org/abs/2004.06774\" target=\"_blank\"><strong>Standardizing and Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing,</strong></a> In arxiv, 2020. [<a href=\"crisis_dataset_bib.html\">Bibtex</a>]</li>-->\n",
    "        <li>Firoj Alam, Ferda Ofli and Muhammad Imran. CrisisMMD: Multimodal Twitter Datasets from Natural Disasters. In Proceedings of the International AAAI Conference on Web and Social Media (ICWSM), 2018, Stanford, California, USA.</li>\n",
    "        <li>Muhammad Imran, Prasenjit Mitra, and Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portorož, Slovenia.</li>\n",
    "        <li>A. Olteanu, S. Vieweg, C. Castillo. 2015. What to Expect When the Unexpected Happens: Social Media Communications Across Crises. In Proceedings of the ACM 2015 Conference on Computer Supported Cooperative Work and Social Computing (CSCW '15). ACM, Vancouver, BC, Canada.</li>\n",
    "        <li>A. Olteanu, C. Castillo, F. Diaz, S. Vieweg. 2014. CrisisLex: A Lexicon for Collecting and Filtering Microblogged Communications in Crises. In Proceedings of the AAAI Conference on Weblogs and Social Media (ICWSM'14). AAAI Press, Ann Arbor, MI, USA.</li>\n",
    "        <li>Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz and Patrick Meier. Extracting Information Nuggets from Disaster-Related Messages in Social Media. In Proceedings of the 10th International Conference on Information Systems for Crisis Response and Management (ISCRAM), May 2013, Baden-Baden, Germany.</li>\n",
    "        <li>Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz and Patrick Meier. Practical Extraction of Disaster-Relevant Information from Social Media. In Social Web for Disaster Management (SWDM'13) - Co-located with WWW, May 2013, Rio de Janeiro, Brazil.</li>\n",
    "        <li>https://appen.com/datasets/combined- disaster-response-data/</li>\n",
    "        <li>https://data.world/crowdflower/disasters- on-social-media</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f0a9b",
   "metadata": {},
   "source": [
    "### Pull text into notebook and Perform Some Exploratory Anaylsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folders\n",
    "labled_data_folder  =  os.path.join(parent_dir,\"Data/crisis_datasets_benchmarks/all_data_en\")\n",
    "initial_filtering_folder = os.path.join(parent_dir,\"Data/crisis_datasets_benchmarks/initial_filtering\")\n",
    "self_pull_folder = os.path.join(parent_dir,\"Data/scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd835de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish dataframes\n",
    "\n",
    "train =  pd.read_table(os.path.join\n",
    "                       (labled_data_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang_en_train.tsv\"))\n",
    "test  =  pd.read_table(os.path.join\n",
    "                       (labled_data_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang_en_test.tsv\"),\n",
    "                       sep ='\\t', quoting =3)\n",
    "dev =  pd.read_table(os.path.join\n",
    "                     (labled_data_folder,\n",
    "                                  \"crisis_consolidated_informativeness_filtered_lang_en_dev.tsv\"))\n",
    "filtered  = pd.read_table(os.path.join\n",
    "                       (initial_filtering_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang.tsv\"))\n",
    "english = filtered[filtered[\"lang\"] == 'en']\n",
    "\n",
    "\n",
    "# geotweets = pd.read_csv(os.path.join(self_pull_folder,\"tweetsid.csv\"))\n",
    "# nogeotweets = pd.read_csv(os.path.join(self_pull_folder,\"tweets_no_geo.csv\"))\n",
    "dflist = [train, test, dev, filtered, english ]\n",
    "\n",
    "combinedf = pd.concat(dflist[0:3])\n",
    "dflist.append(combinedf)\n",
    "dfnames = ['train', 'test', 'dev', 'filtered', 'english', 'combinedf']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406de72",
   "metadata": {},
   "source": [
    "The twitter data were availible in multiple .tsv files. The filitered data .tsv has duplicate tweets (based on twiiter id), and tweets with the same text removed, and has tweets from multiple languages included. Pre spilt .tsv files were included in the downloaded data set. \n",
    "\n",
    "The english data frame selects just the English language tweets from the filtered data set. Interestingly, the train/test/dev data do not have an identical count as the english filtered dataset. \n",
    "\n",
    "As noted below there are 61 total events observed in the overall data set. One event is not present in either the test or dev data. \n",
    "\n",
    "All tweets are in english. \n",
    "\n",
    "Below are exploratory statistics to see if there is a significant difference in variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#itertate over the the data frames and pull value counts for the whole dataset\n",
    "\n",
    "valuecountdfs_all_data=[]\n",
    "valuecountnames_all_data=[]\n",
    "\n",
    "for i, df in enumerate(dflist):\n",
    "    print(\"For the {0} dataframe the shape is: {1}\".format(dfnames[i],dflist[i].shape))\n",
    "    print(\"Data Types for {0}:\\n{1}\".format(dfnames[i],dflist[i].dtypes))\n",
    "    for j, col in enumerate(dflist[i].select_dtypes(include=['object'])):\n",
    "        if col.lower() in  ('event'):\n",
    "            print(\"There are {0} unique {1} for the {2} dataframe\".format(len(dflist[i][col].unique()),col,dfnames[i]))              \n",
    "            name= [col,dfnames[i]]\n",
    "\n",
    "            valuecountdfs_all_data.append(pd.DataFrame(dflist[i][col].value_counts()))\n",
    "            valuecountnames_all_data.append(name)\n",
    "        else:\n",
    "            print(\"There are {0} unique {1} for the {2} dataframe\".format(len(dflist[i][col].unique()),col,dfnames[i]))              \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b92c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Add your code in the Cells below, add more cells if necessary\n",
    "# ----------------------------------------------------------------\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for k, df in enumerate(valuecountdfs_all_data):\n",
    "    plt.subplot(2,3,1+k)\n",
    "    plt.hist(                 # Use the histogram function\n",
    "        valuecountdfs_all_data[k],            # Select a column of data from the dataframe to plot\n",
    "        bins=len(valuecountdfs_all_data[k]),             # Parameterize the number of buckets to collect data into\n",
    "    #     normed=1,             # Normalize the counts (1 = yes, 0 = no) into a portion of 1.0 (aka 100%)\n",
    "        facecolor='green',    # Define the color of the plotted elements\n",
    "        alpha=0.75,           # Define the transparency of the plotted elements\n",
    "        edgecolor=\"k\"         # Define bin edge color (k = black)\n",
    "        )\n",
    "    # Add a label to the X-axis\n",
    "    plt.xlabel(\"Dataframe: {0} Variable: tweets per {1}\".format(valuecountnames_all_data[k][1],valuecountnames_all_data[k][0]))\n",
    "    plt.ylabel(\"count\")  \n",
    "\n",
    "\n",
    "    # Render the plot\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1bf593",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuecountdfs_all_data2=[]\n",
    "valuecountnames_all_data2=[]\n",
    "\n",
    "for i, df in enumerate(dflist):\n",
    "    df = dflist[i].groupby('class_label')\n",
    "#     print(\"For the {0} dataframe the shape is: {1}\".format(dfnames[i],df.shape))\n",
    "    print(\"Data Types for {0}:\\n{1}\".format(dfnames[i],df.dtypes))           \n",
    "    name= [col,dfnames[i]]\n",
    "    valuecountdfs_all_data2.append(df)\n",
    "    valuecountnames_all_data2.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29527cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Add your code in the Cells below, add more cells if necessary\n",
    "# ----------------------------------------------------------------\n",
    "plt.figure(figsize=(24, 16))\n",
    "\n",
    "for k, df in enumerate(valuecountdfs_all_data2):\n",
    "    plt.subplot(2,3,1+k)\n",
    "    plt.hist(                 # Use the histogram function\n",
    "        valuecountdfs_all_data[k],            # Select a column of data from the dataframe to plot\n",
    "        bins=len(valuecountdfs_all_data[k]),             # Parameterize the number of buckets to collect data into\n",
    "        facecolor='orange',    # Define the color of the plotted elements\n",
    "        alpha=0.25,           # Define the transparency of the plotted elements\n",
    "        edgecolor=\"k\"         # Define bin edge color (k = black)\n",
    "        )    \n",
    "    plt.hist(                 # Use the histogram function\n",
    "        valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts(),            # Select a column of data from the dataframe to plot\n",
    "        bins=len(valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts()),             # Parameterize the number of buckets to collect data into\n",
    "        facecolor='green',    # Define the color of the plotted elements\n",
    "        alpha=0.25,           # Define the transparency of the plotted elements\n",
    "        edgecolor=\"k\"         # Define bin edge color (k = black)\n",
    "        )\n",
    "    plt.hist(                 # Use the histogram function\n",
    "        valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts(),            # Select a column of data from the dataframe to plot\n",
    "        bins=len(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts()),             # Parameterize the number of buckets to collect data into\n",
    "        facecolor='blue',    # Define the color of the plotted elements\n",
    "        alpha=0.25,           # Define the transparency of the plotted elements\n",
    "        edgecolor=\"k\"         # Define bin edge color (k = black)\n",
    "        )    \n",
    "    plt.xlabel(\"Dataframe: {0} Variable: tweets per {1}\".format(valuecountnames_all_data[k][1],valuecountnames_all_data[k][0]))\n",
    "    plt.ylabel(\"event type count\")  \n",
    "\n",
    "\n",
    "    # Render the plot\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e791680",
   "metadata": {},
   "outputs": [],
   "source": [
    "l =4\n",
    "contingency = pd.crosstab(dflist[l]['event'], dflist[l]['class_label'])\n",
    "contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.chi2_contingency(contingency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e196f",
   "metadata": {},
   "source": [
    "#### Learning how to do some text stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b83aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproccessed_tweets_train = process(train['text'])\n",
    "preproccessed_tweets_test = process(test['text'])\n",
    "preproccessed_tweets_dec = process(dev['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bad8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproccessed_tweets_geotweets = process(geotweets ['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bf486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test['processed2'] = preproccessed_tweets_train\n",
    "train['processed2'] = preproccessed_tweets_test\n",
    "dev['processed2'] = preproccessed_tweets_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotweets['processed2'] = preproccessed_tweets_geotweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ddf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8655cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "train_counts = cv.fit_transform(train['processed'])\n",
    "test_data = cv.transform(test['processed'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "clf = nb.fit(train_counts, train['class_label'])\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "train_counts = cv.fit_transform(train['processed'])\n",
    "test_data = cv.transform(geotweets['processed2'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "clf = nb.fit(train_counts, train['class_label'])\n",
    "predicted_geo = clf.predict(test_data)\n",
    "\n",
    "\n",
    "# print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992922ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotweets['predict']=predicted_geo\n",
    "print(geotweets.iloc[244]['Text'] )\n",
    "print (geotweets.iloc[244]['processed2'])\n",
    "print (geotweets.iloc[244]['predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read about Pipelines here:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "clf = clf.fit(train['processed'], train['class_label'])\n",
    "predicted = clf.predict(test['processed'])\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed'], test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tools = [('tf', TfidfVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "# set_params() of TfidfVectorizer below, sets the parameters of the estimator. The method works on simple estimators as \n",
    "# well as on nested objects (such as pipelines). The pipelines have parameters of the form <component>__<parameter> \n",
    "# so that it’s possible to update each component of a nested object.\n",
    "clf.set_params(tf__stop_words = 'english')\n",
    "\n",
    "clf = clf.fit(train['processed'], train['class_label'])\n",
    "predicted = clf.predict(test['processed'])\n",
    "\n",
    "print(\"NB (TF-IDF with Stop Words) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed'], test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea81a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "train_counts = cv.fit_transform(train['processed2'])\n",
    "test_data = cv.transform(test['processed2'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "clf = nb.fit(train_counts, train['class_label'])\n",
    "predicted = clf.predict(test_data)\n",
    "\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test_data, test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read about Pipelines here:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "clf = clf.fit(train['processed2'], train['class_label'])\n",
    "predicted = clf.predict(test['processed2'])\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed2'], test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e60558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tools = [('tf', TfidfVectorizer()), ('nb', MultinomialNB())]\n",
    "clf = Pipeline(tools)\n",
    "\n",
    "# set_params() of TfidfVectorizer below, sets the parameters of the estimator. The method works on simple estimators as \n",
    "# well as on nested objects (such as pipelines). The pipelines have parameters of the form <component>__<parameter> \n",
    "# so that it’s possible to update each component of a nested object.\n",
    "clf.set_params(tf__stop_words = 'english')\n",
    "\n",
    "clf = clf.fit(train['processed2'], train['class_label'])\n",
    "predicted = clf.predict(test['processed2'])\n",
    "\n",
    "print(\"NB (TF-IDF with Stop Words) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed2'], test['class_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(stop_words = 'english')),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('lr', LogisticRegression())])\n",
    "\n",
    "\n",
    "clf = clf.fit(train['processed2'], train['class_label'])\n",
    "predicted = clf.predict(test['processed2'])\n",
    "predicted_geo_lab = clf.predict(geotweets['processed2'])\n",
    "geotweets['predict']=predicted_geo_lab\n",
    "\n",
    "print(\"LR (TF-IDF with Stop Words) prediction accuracy = {0:5.1f}%\".format(100.0 * clf.score(test['processed2'], test['class_label'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24843cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Generate 10 random numbers between 0 and 30\n",
    "randomlist = random.sample(range(0, len(geotweets)), 10)\n",
    "print(randomlist)\n",
    "for z in randomlist:\n",
    "    print(\"Predicted Label {0}\".format(geotweets.iloc[z]['predict']))\n",
    "    print(\"initial tweet {0}\".format(geotweets.iloc[z]['Text']))\n",
    "    print(\"Processed tweet {0}\".format(geotweets.iloc[z]['processed2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase and restrict ourselves to about half the available features\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(train['processed2'], train['class_label'])\n",
    "y_pred = pclf.predict(test['processed2'])\n",
    "print(metrics.classification_report(test['class_label'], y_pred, target_names = ['informative','not']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import scale, LabelBinarizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Random seed for numpy\n",
    "np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11382e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Build a mode that is composed of this list of layers\n",
    "model = Sequential(\n",
    "    [\n",
    "          # This specifies a single neuron, and the input is 2 numbers.\n",
    "    Dense(1, input_dim=2),  # a dense layer, every neuron is connected to all points from the lower layer (input)\n",
    "    Activation('linear'),   # Specify the type of decision surface, i.e., simple linear regression\n",
    "    Dense(1),               # another dense layer, input_dim is inferred from the previous layer's output\n",
    "    Activation('sigmoid')   # Specify the type of decision surface, i.e., simple logistic regression\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44837b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 9\n",
    "message = english.iloc[n]['text']\n",
    "message_procc =  preproccessed_tweets [n]\n",
    "print(message)\n",
    "print(message_procc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00982481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate counts of tokens, using string functionality\n",
    "\n",
    "\n",
    "# Used to print sequences in a nice manner\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80,compact=True)\n",
    "\n",
    "# Tokenize the message and create a counter for frequency of each word in message.\n",
    "# Browse for split() python or go to this link http://www.pythonforbeginners.com/dictionary/python-split to see what split() does\n",
    "words = message.split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulate counts of tokens, using string functionality\n",
    "\n",
    "\n",
    "# Used to print sequences in a nice manner\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80,compact=True)\n",
    "\n",
    "# Tokenize the message and create a counter for frequency of each word in message.\n",
    "# Browse for split() python or go to this link http://www.pythonforbeginners.com/dictionary/python-split to see what split() does\n",
    "words = message_procc.split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f6c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = message.lower().split()\n",
    "word_count = col.Counter(words)\n",
    "\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "twks= twokenize.tokenizeRawTweetText(message)\n",
    "word_count = col.Counter(twks)\n",
    "# Setting the limit to 40 for the number of tokens to display \n",
    "counts_to_display = 40\n",
    "\n",
    "# Display results. \n",
    "print('Total number of tokens = {0}'.format(len(word_count)))\n",
    "print(30*'-')\n",
    "print('Top {} tokens by frequency:'.format(counts_to_display))\n",
    "print(30*'-')\n",
    "pp.pprint(word_count.most_common(counts_to_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the below print statememt, {0:12s} means, print argument 1 with 12 spaces allocated for it. \n",
    "# You can see that for two of the results, 12 spaces is not adequate and these results are misaligned.\n",
    "print('{0:12s}: {1}'.format('Term', 'Frequency'))\n",
    "print(20*'-')\n",
    "\n",
    "total_word_count = sum(word_count.values())\n",
    "for count in word_count.most_common(counts_to_display):\n",
    "    pp.pprint('{0:12s}: {1:4.3f}'.format(count[0], count[1]/total_word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8efc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer='word', lowercase=True)\\\n",
    "cv.fit(english['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now process documents.\n",
    "\n",
    "# We need an iteratable to apply cv.transform()\n",
    "msg = []\n",
    "msg.append(message)\n",
    "\n",
    "# Transforming a single message is easier to comprehend. By default, scikit learn uses sparse matrices for text processing\n",
    "# It returns a Document Term Matrix (dtm)\n",
    "dtm = cv.transform(msg)\n",
    "\n",
    "# In sparse format number of tokens indicate size of dataset vocabulary. \n",
    "# So there is 1 document and 130107 featues in the dtm.\n",
    "print('Number of Samples = {0}'.format(dtm.shape[0]))\n",
    "print('Number of Tokens = {0}'.format(dtm.shape[1]))\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "# You can't explore the document-term matrix when it is in sparse form. We can convert from sparse to dense form to explore \n",
    "# the document-term matrix. The range given below is chosen randomly. \n",
    "# Each word is a feature. Below zeros indicate the words/features in columns 1000 to 1100, those words do not appear in \n",
    "# the input message. Thats why we have zeros for those cells\n",
    "print(dtm.todense()[:,1000:1100])\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "# We can also print only nonzero DTM matrix elements. \n",
    "print('Cells from Document-Term Matrix[i, j] and c (Count)')\n",
    "print(80*'-')\n",
    "\n",
    "\n",
    "\n",
    "# Find non-zero elements. scipy.sparse.find() returns the indices and values of the nonzero elements of a matrix.\n",
    "# i,j contains the row and column indices where non zero matrix entries are present while V has the entry's value.\n",
    "i, j, V = sp.find(dtm)\n",
    "dtm_list = list(zip(i, j, V))\n",
    "pp.pprint(dtm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bad798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.vocabulary_[\"monsoon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b703ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtm_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dtm_list, key=itemgetter(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the terms in the vocabulary\n",
    "terms = cv.vocabulary_\n",
    "\n",
    "# Look for a single term confuse\n",
    "search_word = 'blizzard'\n",
    "print(\"Chosen Word ({0}): Column = {1}\".format(search_word, terms[search_word]))\n",
    "\n",
    "# Find the maximum value in dtm_list in 3rd column which will be 114455\n",
    "max_key = max(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "# Find the minimum value in dtm_list in 3rd column which will be 2336\n",
    "min_key = min(dtm_list, key=itemgetter(2))[1]\n",
    "\n",
    "# In the below two lines, terms.keys() will return all keys - i.e. the column names(words).\n",
    "# For loop iterates over all this words to see get the column name which matches the column index we have in max_key and min_key\n",
    "x_max = [key for key in terms.keys() if terms[key] == max_key]\n",
    "x_min = [key for key in terms.keys() if terms[key] == min_key]\n",
    "\n",
    "# the for loop above returned a list as output. So x_max is a list with a column name as same with x_min\n",
    "print(\"Max Word ({0}): Column = {1}\".format(x_max[0], max_key))\n",
    "print(\"Min Word ({0}): Column = {1}\".format(x_min[0], min_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd50d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize a text document\n",
    "# word_tokenize() is tokenizing the message and each word is being converted to lowercase. \n",
    "# So words has the vocabulary of message\n",
    "words = [word.lower() for word in nltk.word_tokenize(message)]\n",
    "top_display=25\n",
    "\n",
    "# Count number of occurances for each token\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f99ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a text document\n",
    "# word_tokenize() is tokenizing the message and each word is being converted to lowercase. \n",
    "# So words has the vocabulary of message\n",
    "words = [word.lower() for word in nltk.word_tokenize(message_procc)]\n",
    "top_display=25\n",
    "\n",
    "# Count number of occurances for each token\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Regular Expression to parse a text document\n",
    "\n",
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', message_procc))]\n",
    "\n",
    "# Count token occurances\n",
    "counts = nltk.FreqDist(words)\n",
    "pp.pprint(counts.most_common(top_display))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aabbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(words)\n",
    "num_tokens = len(counts)\n",
    "lexdiv  =  num_words / num_tokens\n",
    "print(\"Message has %i tokens and %i words for a lexical diversity of %0.3f\" % (num_tokens, num_words, lexdiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of unique tokens (or bins)\n",
    "print('Number of unique bins(tokens) = {0}\\n'.format(counts.B()))\n",
    "print('Number of sample outcomes = {0}\\n'.format(counts.N()))\n",
    "print('Maximum occuring token = {0}\\n'.format(counts.max()))\n",
    "\n",
    "print('{0:12s}: {1}'.format('Term', 'Count'))\n",
    "print(25*'-')\n",
    "\n",
    "for token, freq in counts.most_common(top_display):\n",
    "    print('{0:12s}:  {1:4.3f}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapaxes\n",
    "pp.pprint(counts.hapaxes()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of elements to display\n",
    "top_display=10\n",
    "counts.tabulate(top_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5befe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(10,6))\n",
    "sns.set(style=\"white\", font_scale=1.5)\n",
    "sns.despine(offset=5)#, trim=True)\n",
    "counts.plot(top_display, cumulative=True)\n",
    "axs.set_title('Term Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498774b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentance to tokenize\n",
    "my_text = message_procc\n",
    "\n",
    "cv1 = CountVectorizer(lowercase=True)\n",
    "cv2 = CountVectorizer(stop_words = 'english', lowercase=True)\n",
    "\n",
    "tk_func1 = cv1.build_analyzer()\n",
    "tk_func2 = cv2.build_analyzer()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "print('Tokenization:')\n",
    "pp.pprint(tk_func1(my_text))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Tokenization (with Stop words):')\n",
    "pp.pprint(tk_func2(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cedaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_func1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46230f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_text = message_procc\n",
    "stemmer = PorterStemmer()\n",
    "tokens = nltk.word_tokenize(new_text)\n",
    "tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "for w in tokens:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2690559",
   "metadata": {},
   "outputs": [],
   "source": [
    "english['text']0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49c7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
