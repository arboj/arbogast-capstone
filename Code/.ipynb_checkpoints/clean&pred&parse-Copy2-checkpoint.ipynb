{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7cdda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "since_date = '2021-07-11'\n",
    "until_date = '2021-07-12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import datetime\n",
    "from capstone_twitter_search import twittsearch\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np                               # linear algebra\n",
    "import pandas as pd                              # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re                                        # to handle regular expressions\n",
    "from string import punctuation                   # to extract the puntuation symbols\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize          # to divide strings into tokens\n",
    "from nltk.stem import WordNetLemmatizer          # to lemmatize the tokens\n",
    "from nltk.corpus import stopwords                # to remove the stopwords \n",
    "\n",
    "import random                                    # for generating (pseudo-)random numbers\n",
    "import matplotlib.pyplot as plt                  # to plot some visualizations\n",
    "import datetime\n",
    "import tensorflow as tf            \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from mordecai import Geoparser\n",
    "#from mordecai import batch_geoparse\n",
    "from flatten_json import flatten_json\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras import layers\n",
    "import text_proccessing\n",
    "from text_proccessing import clean_text\n",
    "from text_proccessing import remove_stopwords\n",
    "from text_proccessing import lemmatize_text\n",
    "from text_proccessing import concatenate_text\n",
    "from text_proccessing import makeglove\n",
    "from text_proccessing import make_embedding_matrix\n",
    "import append_df\n",
    "from append_df import append_df\n",
    "import modhelp\n",
    "from modhelp import train_val_split\n",
    "from modhelp import geo_df\n",
    "from modhelp import suggest_nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = os.getcwd()\n",
    "\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "data_dir = os.path.join(parent_dir,\"Data\")\n",
    "dsa= os.path.dirname(parent_dir)\n",
    "tweet_dir = os.path.join(parent_dir,\"TweetMap\")\n",
    "directory = os.path.join(data_dir,'splits')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6089ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(directory, 'InformativenessTrain_Tokenized.csv'),engine='python')\n",
    "train_data['text']= train_data['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =  datetime.datetime.now() \n",
    "print('splitthis started at: {}'.format(start))\n",
    "train_samples, val_samples, train_labels, val_labels = train_val_split(train_data, 0.25)\n",
    "end =  datetime.datetime.now() \n",
    "print ('Text done {}'.format(end-start))\n",
    "start =  datetime.datetime.now() \n",
    "print('Loading model,embedding and glove commenced at  {}'.format(start))\n",
    "\n",
    "model = tf.keras.models.load_model(os.path.join(dsa,'model1'))\n",
    "print(\"loading vecotor\")\n",
    "path_to_glove_file = os.path.join(dsa,'WordVector','glove.twitter.27B.200d.txt')\n",
    "\n",
    "print(\"indexing\")\n",
    "embeddings_index=makeglove(path_to_glove_file)\n",
    "print(\"matrix and vectorize\")\n",
    "embedding_matrix, vectorizer = make_embedding_matrix(train_samples, val_samples, embeddings_index)\n",
    "end =  datetime.datetime.now() \n",
    "\n",
    "print(\"embedded in {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "twts = pd.read_csv(os.path.join(tweet_dir,'tweets_df_{0}_{1}.csv'.\n",
    "                              format(since_date.replace('-',''),until_date.replace('-',''))), engine = 'python')\n",
    "twts['Text'] = twts['Text'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "procstart =  datetime.datetime.now()\n",
    "print(\"{}: processing \".format(procstart))\n",
    "twts['ptext'] = twts['Text'].apply(lambda x: clean_text(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x: word_tokenize(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : remove_stopwords(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : lemmatize_text(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : concatenate_text(x))\n",
    "procend=  datetime.datetime.now() \n",
    "print(\"{}: processed in {}\".format(procend, procend - procstart))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8933ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predstart = datetime.datetime.now() \n",
    "print(\"{}: predict\".format(predstart))\n",
    "predictions = suggest_nn2(twts, model,vectorizer)\n",
    "\n",
    "submission_data = {\"ID\": twts['TweetId'].tolist(),\"tweet\": twts['Text'].tolist(), \"target\": predictions}\n",
    "\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "result = twts.join(submission_df.target)\n",
    "predend = datetime.datetime.now() \n",
    "print(\"predicted {}\".format(predend-predstart))\n",
    "# \n",
    "result_inf = result[result['target']==0]\n",
    "print(\"working over {} informative tweets\".format(len(result_inf)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49897543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Run geoprocessing over tweets\")\n",
    "\n",
    "geostart =  datetime.datetime.now() \n",
    "print(\"{}: geostart\".format(geostart))\n",
    "df_js = geo_df(result_inf)\n",
    "geoend =  datetime.datetime.now()\n",
    "print(\"{}: geoend {}\".format(geoend,geoend-geostart))\n",
    "\n",
    "df_js.to_csv(os.path.join(tweet_dir,'result_{0}_{1}.csv'.\n",
    "                              format(since_date.replace('-',''),until_date.replace('-',''))))\n",
    "print(\"f√≠n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
