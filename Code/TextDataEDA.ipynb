{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0543eb34",
   "metadata": {},
   "source": [
    "# Text Conditioning and Machine Learning\n",
    "\n",
    "This notebook lays out the process of some exploratory data analysis for the twitter disaster data pulled. This is to get a sense of the data whcih will be used for training, and to idetify potential hinderances going forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1859dc",
   "metadata": {},
   "source": [
    "## Python Packages Used\n",
    "This notebook was set up in an environment running Python 3.8 with the following packages:\n",
    "pandas, tensorflow, keras, scikit-learn, nltk, gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b272dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import collections as col\n",
    "import pprint\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import twokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy.stats as stats\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8dc5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Elements/DataScience/dsa/capstone\n"
     ]
    }
   ],
   "source": [
    "code_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a19dc9",
   "metadata": {},
   "source": [
    "## [Crisis Benchmark data for training Models](https://crisisnlp.qcri.org/crisis_datasets_benchmarks.html)\n",
    "\n",
    "<p>The crisis benchmark dataset consists data from several different data sources such as CrisisLex (<a href=\"http://crisislex.org/data-collections.html#CrisisLexT26\" target=\"_blank\">CrisisLex26</a>, <a href=\"http://crisislex.org/data-collections.html#CrisisLexT6\" target=\"_blank\">CrisisLex6</a>), <a href=\"https://crisisnlp.qcri.org/lrec2016/lrec2016.html\" target=\"_blank\">CrisisNLP</a>, <a href=\"http://mimran.me/papers/imran_shady_carlos_fernando_patrick_practical_2013.pdf\" target=\"_blank\">SWDM2013</a>, <a href=\"http://mimran.me/papers/imran_shady_carlos_fernando_patrick_iscram2013.pdf\" target=\"_blank\">ISCRAM13</a>, Disaster Response Data (DRD), <a href=\"https://data.world/crowdflower/disasters-on-social-media\" target=\"_blank\">Disasters on Social Media (DSM)</a>, <a href=\"https://crisisnlp.qcri.org/crisismmd\" target=\"_blank\">CrisisMMD</a> and data from <a href=\"http://aidr.qcri.org/\" target=\"_blank\">AIDR</a>. \n",
    "\t  The class label was mapped, remove duplicates removed and this was provided as a benchmark results for the community. </p>\n",
    "\n",
    "The authors have their model and data availible on github at <a href=\"https://github.com/firojalam/crisis_datasets_benchmarks\">https://github.com/firojalam/crisis_datasets_benchmarks</a>    </p>\n",
    "\n",
    "#### Data Availible from: https://crisisnlp.qcri.org/data/crisis_datasets_benchmarks/crisis_datasets_benchmarks_v1.0.tar.gz\n",
    "<h4><strong>References</strong></h4>\n",
    "<ol>\n",
    "<li><a href=\"http://sites.google.com/site/firojalam/\">Firoj Alam</a>, <a href=\"https://hsajjad.github.io/\">Hassan Sajjad</a>, <a href=\"http://mimran.me/\">Muhammad Imran</a> and <a href=\"https://sites.google.com/site/ferdaofli/\">Ferda Ofli</a>, <a href=\"https://arxiv.org/abs/2004.06774\" target=\"_blank\"><strong>CrisisBench: Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing,</strong></a> In ICWSM, 2021. [<a href=\"crisis_dataset_bib1.html\">Bibtex</a>]\n",
    "        </li>\n",
    "<!-- <li><a href=\"http://sites.google.com/site/firojalam/\">Firoj Alam</a>, <a href=\"https://hsajjad.github.io/\">Hassan Sajjad</a>, <a href=\"http://mimran.me/\">Muhammad Imran</a> and <a href=\"https://sites.google.com/site/ferdaofli/\">Ferda Ofli</a>, <a href=\"https://arxiv.org/abs/2004.06774\" target=\"_blank\"><strong>Standardizing and Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing,</strong></a> In arxiv, 2020. [<a href=\"crisis_dataset_bib.html\">Bibtex</a>]</li>-->\n",
    "        <li>Firoj Alam, Ferda Ofli and Muhammad Imran. CrisisMMD: Multimodal Twitter Datasets from Natural Disasters. In Proceedings of the International AAAI Conference on Web and Social Media (ICWSM), 2018, Stanford, California, USA.</li>\n",
    "        <li>Muhammad Imran, Prasenjit Mitra, and Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portoro≈æ, Slovenia.</li>\n",
    "        <li>A. Olteanu, S. Vieweg, C. Castillo. 2015. What to Expect When the Unexpected Happens: Social Media Communications Across Crises. In Proceedings of the ACM 2015 Conference on Computer Supported Cooperative Work and Social Computing (CSCW '15). ACM, Vancouver, BC, Canada.</li>\n",
    "        <li>A. Olteanu, C. Castillo, F. Diaz, S. Vieweg. 2014. CrisisLex: A Lexicon for Collecting and Filtering Microblogged Communications in Crises. In Proceedings of the AAAI Conference on Weblogs and Social Media (ICWSM'14). AAAI Press, Ann Arbor, MI, USA.</li>\n",
    "        <li>Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz and Patrick Meier. Extracting Information Nuggets from Disaster-Related Messages in Social Media. In Proceedings of the 10th International Conference on Information Systems for Crisis Response and Management (ISCRAM), May 2013, Baden-Baden, Germany.</li>\n",
    "        <li>Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz and Patrick Meier. Practical Extraction of Disaster-Relevant Information from Social Media. In Social Web for Disaster Management (SWDM'13) - Co-located with WWW, May 2013, Rio de Janeiro, Brazil.</li>\n",
    "        <li>https://appen.com/datasets/combined- disaster-response-data/</li>\n",
    "        <li>https://data.world/crowdflower/disasters- on-social-media</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f0a9b",
   "metadata": {},
   "source": [
    "### Pull text into notebook and Perform Some Exploratory Anaylsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3f666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folders\n",
    "labled_data_folder  =  os.path.join(parent_dir,\"Data/crisis_datasets_benchmarks/all_data_en\")\n",
    "initial_filtering_folder = os.path.join(parent_dir,\"Data/crisis_datasets_benchmarks/initial_filtering\")\n",
    "self_pull_folder = os.path.join(parent_dir,\"Data/scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd835de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish dataframes\n",
    "\n",
    "train =  pd.read_table(os.path.join\n",
    "                       (labled_data_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang_en_train.tsv\"))\n",
    "test  =  pd.read_table(os.path.join\n",
    "                       (labled_data_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang_en_test.tsv\"),\n",
    "                       sep ='\\t', quoting =3)\n",
    "dev =  pd.read_table(os.path.join\n",
    "                     (labled_data_folder,\n",
    "                                  \"crisis_consolidated_informativeness_filtered_lang_en_dev.tsv\"))\n",
    "filtered  = pd.read_table(os.path.join\n",
    "                       (initial_filtering_folder,\n",
    "                                    \"crisis_consolidated_informativeness_filtered_lang.tsv\"))\n",
    "english = filtered[filtered[\"lang\"] == 'en']\n",
    "\n",
    "\n",
    "# geotweets = pd.read_csv(os.path.join(self_pull_folder,\"tweetsid.csv\"))\n",
    "# nogeotweets = pd.read_csv(os.path.join(self_pull_folder,\"tweets_no_geo.csv\"))\n",
    "dflist = [train, test, dev, filtered, english ]\n",
    "\n",
    "combinedf = pd.concat(dflist[0:3])\n",
    "dflist.append(combinedf)\n",
    "dfnames = ['train', 'test', 'dev', 'filtered', 'english', 'combinedf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ab9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedf.to_csv(os.path.join(labled_data_folder,\"combinedf.csv\"),index_label = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406de72",
   "metadata": {},
   "source": [
    "The twitter data were availible in multiple .tsv files. The filitered data .tsv has duplicate tweets (based on twiiter id), and tweets with the same text removed, and has tweets from multiple languages included. Pre spilt .tsv files were included in the downloaded data set. \n",
    "\n",
    "The english data frame selects just the English language tweets from the filtered data set. Interestingly, the train/test/dev data do not have an identical count as the english filtered dataset. \n",
    "\n",
    "As noted below there are 61 total events observed in the overall data set. One event is not present in either the test or dev data. \n",
    "\n",
    "All tweets are in english. \n",
    "\n",
    "Below are exploratory statistics to see if there is a significant difference in variables, how does the distribution of events change between the train, test, dev samples, and the informativeness of the tweet. The goal is to gain an understanding how much influence the humanitarian event has on the classification of the usefullness\n",
    "\n",
    "\n",
    "The first plots will break out counts of tweets for each humanitarian crisis for each data set (train, test, dev; filtered, just english, and the combination of the train test dev data) to see the distrubtuin. Afterwards, the data are spilt by the \"class_label\" variable to determine, what if any differecne there is between the distrubition of tweets by events of the informative vs not informative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8c9945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the train dataframe the shape is: (109441, 7)\n",
      "Data Types for train:\n",
      "id               int64\n",
      "event           object\n",
      "source          object\n",
      "text            object\n",
      "lang            object\n",
      "lang_conf      float64\n",
      "class_label     object\n",
      "dtype: object\n",
      "There are 61 unique event for the train dataframe\n",
      "There are 10 unique source for the train dataframe\n",
      "There are 109441 unique text for the train dataframe\n",
      "There are 1 unique lang for the train dataframe\n",
      "informative        65612\n",
      "not_informative    43829\n",
      "Name: class_label, dtype: int64\n",
      "\n",
      "\n",
      "For the test dataframe the shape is: (31095, 7)\n",
      "Data Types for test:\n",
      "id               int64\n",
      "event           object\n",
      "source          object\n",
      "text            object\n",
      "lang            object\n",
      "lang_conf      float64\n",
      "class_label     object\n",
      "dtype: object\n",
      "There are 60 unique event for the test dataframe\n",
      "There are 10 unique source for the test dataframe\n",
      "There are 31095 unique text for the test dataframe\n",
      "There are 1 unique lang for the test dataframe\n",
      "informative        18626\n",
      "not_informative    12469\n",
      "Name: class_label, dtype: int64\n",
      "\n",
      "\n",
      "For the dev dataframe the shape is: (15870, 7)\n",
      "Data Types for dev:\n",
      "id               int64\n",
      "event           object\n",
      "source          object\n",
      "text            object\n",
      "lang            object\n",
      "lang_conf      float64\n",
      "class_label     object\n",
      "dtype: object\n",
      "There are 60 unique event for the dev dataframe\n",
      "There are 10 unique source for the dev dataframe\n",
      "There are 15870 unique text for the dev dataframe\n",
      "There are 1 unique lang for the dev dataframe\n",
      "informative        9506\n",
      "not_informative    6364\n",
      "Name: class_label, dtype: int64\n",
      "\n",
      "\n",
      "For the filtered dataframe the shape is: (165650, 7)\n",
      "Data Types for filtered:\n",
      "id                   int64\n",
      "event               object\n",
      "source              object\n",
      "text                object\n",
      "lang                object\n",
      "lang_confidence    float64\n",
      "class_label         object\n",
      "dtype: object\n",
      "There are 61 unique event for the filtered dataframe\n",
      "There are 10 unique source for the filtered dataframe\n",
      "There are 165650 unique text for the filtered dataframe\n",
      "There are 80 unique lang for the filtered dataframe\n",
      "informative        101440\n",
      "not_informative     64210\n",
      "Name: class_label, dtype: int64\n",
      "\n",
      "\n",
      "For the english dataframe the shape is: (156452, 7)\n",
      "Data Types for english:\n",
      "id                   int64\n",
      "event               object\n",
      "source              object\n",
      "text                object\n",
      "lang                object\n",
      "lang_confidence    float64\n",
      "class_label         object\n",
      "dtype: object\n",
      "There are 61 unique event for the english dataframe\n",
      "There are 10 unique source for the english dataframe\n",
      "There are 156452 unique text for the english dataframe\n",
      "There are 1 unique lang for the english dataframe\n",
      "informative        93727\n",
      "not_informative    62725\n",
      "Name: class_label, dtype: int64\n",
      "\n",
      "\n",
      "For the combinedf dataframe the shape is: (156406, 7)\n",
      "Data Types for combinedf:\n",
      "id               int64\n",
      "event           object\n",
      "source          object\n",
      "text            object\n",
      "lang            object\n",
      "lang_conf      float64\n",
      "class_label     object\n",
      "dtype: object\n",
      "There are 61 unique event for the combinedf dataframe\n",
      "There are 10 unique source for the combinedf dataframe\n",
      "There are 156406 unique text for the combinedf dataframe\n",
      "There are 1 unique lang for the combinedf dataframe\n",
      "informative        93744\n",
      "not_informative    62662\n",
      "Name: class_label, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#itertate over the the data frames and pull value counts for the whole dataset\n",
    "\n",
    "valuecountdfs_all_data=[]\n",
    "valuecountnames_all_data=[]\n",
    "\n",
    "for i, df in enumerate(dflist):\n",
    "    print(\"For the {0} dataframe the shape is: {1}\".format(dfnames[i],dflist[i].shape))\n",
    "    print(\"Data Types for {0}:\\n{1}\".format(dfnames[i],dflist[i].dtypes))\n",
    "    for j, col in enumerate(dflist[i].select_dtypes(include=['object'])):\n",
    "        if col.lower() in  ('event'):\n",
    "            print(\"There are {0} unique {1} for the {2} dataframe\".format(len(dflist[i][col].unique()),col,dfnames[i]))              \n",
    "            name= [col,dfnames[i]]\n",
    "\n",
    "            valuecountdfs_all_data.append(pd.DataFrame(dflist[i][col].value_counts()))\n",
    "            valuecountnames_all_data.append(name)\n",
    "        elif col.lower() in  ('class_label'):\n",
    "            \n",
    "            print(dflist[i][col].value_counts())\n",
    "            \n",
    "        else:\n",
    "            print(\"There are {0} unique {1} for the {2} dataframe\".format(len(dflist[i][col].unique()),col,dfnames[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxlist=[]\n",
    "boxname=[]\n",
    "tickl=[]\n",
    "for k, df in enumerate(valuecountdfs_all_data):\n",
    "    boxlist.append(valuecountdfs_all_data[k].reset_index(drop=True).squeeze())\n",
    "    boxname.append(valuecountnames_all_data[k][1])\n",
    "    tickl.append(k+1)\n",
    "    \n",
    "plt.figure(figsize=(15, 5)) \n",
    "\n",
    "plt.boxplot(boxlist) \n",
    "\n",
    "\n",
    "plt.ylabel(\"boxplot\")\n",
    "\n",
    "plt.xticks(tickl, boxname)\n",
    "#plt.ylim(5, 25)\n",
    "plt\n",
    "# boxname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b92c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) Add your code in the Cells below, add more cells if necessary\n",
    "# ----------------------------------------------------------------\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for k, df in enumerate(valuecountdfs_all_data):\n",
    "    plt.subplot(2,3,1+k)\n",
    "    plt.hist(                 # Use the histogram function\n",
    "        valuecountdfs_all_data[k],            # Select a column of data from the dataframe to plot\n",
    "        bins=len(valuecountdfs_all_data[k]),             # Parameterize the number of buckets to collect data into\n",
    "        density=True,             # Normalize the counts (1 = yes, 0 = no) into a portion of 1.0 (aka 100%)\n",
    "        facecolor='green',    # Define the color of the plotted elements\n",
    "        alpha=0.75,           # Define the transparency of the plotted elements\n",
    "        edgecolor=\"k\"         # Define bin edge color (k = black)\n",
    "        )\n",
    "    # Add a label to the X-axis\n",
    "    s, loc, scale = stats.lognorm.fit(valuecountdfs_all_data[k], floc=0)\n",
    "    estimated_mu = np.log(scale)\n",
    "    estimated_sigma = s\n",
    "    mu = valuecountdfs_all_data[k]['event'].mean()\n",
    "    sigma = valuecountdfs_all_data[k]['event'].std()\n",
    "    x=np.arange(min(valuecountdfs_all_data[k]['event']),\n",
    "                max(valuecountdfs_all_data[k]['event']),0.1)\n",
    "    # Calculate and plot a normal distribution Probability Density Function or PDF\n",
    "#     y2 = stats.norm.pdf(x, mu, sigma)  # Alternate: y = stats.norm.pdf(x, mu, sigma)\n",
    "    y = stats.lognorm.pdf(x, s, scale=scale)\n",
    "\n",
    "#     plt.plot(x,y2,'r--')\n",
    "    plt.plot(x,y,'b--')   \n",
    "    \n",
    "    \n",
    "    plt.xlabel(\"Dataframe: {0} Variable: tweets per {1}\".format(valuecountnames_all_data[k][1],valuecountnames_all_data[k][0]))\n",
    "    plt.ylabel(\"count\")  \n",
    "\n",
    "\n",
    "    # Render the plot\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1bf593",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuecountdfs_all_data2=[]\n",
    "valuecountnames_all_data2=[]\n",
    "\n",
    "for i, df in enumerate(dflist):\n",
    "    df = dflist[i].groupby('class_label')\n",
    "#     print(\"For the {0} dataframe the shape is: {1}\".format(dfnames[i],df.shape))\n",
    "    print(\"Data Types for {0}:\\n{1}\".format(dfnames[i],df.dtypes))           \n",
    "    name= [col,dfnames[i]]\n",
    "    valuecountdfs_all_data2.append(df)\n",
    "    valuecountnames_all_data2.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuecountdfs_all_data2=[]\n",
    "valuecountnames_all_data2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29527cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) Add your code in the Cells below, add more cells if necessary\n",
    "# ----------------------------------------------------------------\n",
    "plt.figure(figsize=(24, 16))\n",
    "\n",
    "for k, df in enumerate(valuecountdfs_all_data2):\n",
    "    plt.subplot(2,3,1+k)\n",
    "#     plt.hist(                 # Use the histogram function\n",
    "#         valuecountdfs_all_data[k],            # Select a column of data from the dataframe to plot\n",
    "#         bins=len(valuecountdfs_all_data[k]),             # Parameterize the number of buckets to collect data into\n",
    "#         facecolor='green',    # Define the color of the plotted elements\n",
    "#         alpha=0.25,           # Define the transparency of the plotted elements\n",
    "#         edgecolor=\"k\"         # Define bin edge color (k = black)\n",
    "#         )    \n",
    "    plt.hist(                 # Use the histogram function\n",
    "        valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts(),            # Select a column of data from the dataframe to plot\n",
    "        bins=len(valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts()),             # Parameterize the number of buckets to collect data into\n",
    "        facecolor='orange',    # Define the color of the plotted elements\n",
    "        density=True,\n",
    "        alpha=0.25,           # Define the transparency of the plotted elements\n",
    "        edgecolor=\"k\"         # Define bin edge color (k = black)\n",
    "        \n",
    "        )\n",
    "    plt.hist(                 # Use the histogram function\n",
    "        valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts(),            # Select a column of data from the dataframe to plot\n",
    "        bins=len(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts()),             # Parameterize the number of buckets to collect data into\n",
    "        facecolor='blue',    # Define the color of the plotted elements\n",
    "        density=True,\n",
    "        alpha=0.25,           # Define the transparency of the plotted elements\n",
    "        edgecolor=\"k\"         # Define bin edge color (k = black)\n",
    "        )\n",
    "    \n",
    "    s, loc, scale = stats.lognorm.fit(valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts(), floc=0)\n",
    "    estimated_mu = np.log(scale)\n",
    "    estimated_sigma = s\n",
    "#     mu = valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts()\n",
    "#     sigma = valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts()\n",
    "    x = np.arange(min(valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts()),\n",
    "                max(valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts()),0.1)\n",
    "    # Calculate and plot a normal distribution Probability Density Function or PDF\n",
    "#     y = stats.norm.pdf(x, mu, sigma)  # Alternate: y = stats.norm.pdf(x, mu, sigma)\n",
    "    y = stats.lognorm.pdf(x, s, scale=scale)   \n",
    "\n",
    "    s2, loc2, scale2 = stats.lognorm.fit(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts(), floc=0)\n",
    "    estimated_mu2 = np.log(scale2)\n",
    "    estimated_sigma2 = s2\n",
    "\n",
    "#     mu2 = valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts().mean()\n",
    "#     sigma2 = valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts().std()\n",
    "    x2=np.arange(min(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts()),\n",
    "                max(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts()),0.1)\n",
    "    # Calculate and plot a normal distribution Probability Density Function or PDF\n",
    "#     y2 = stats.norm.pdf(x2, mu2, sigma2)  # Alternate: y = stats.norm.pdf(x, mu, sigma)\n",
    "    maxevent = max(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts())\n",
    "    length = len(valuecountdfs_all_data2[k]['event'].get_group('not_informative'))\n",
    "    plt.ylim(0,.015)\n",
    "    y2 = stats.lognorm.pdf(x2, s2, scale=scale2)\n",
    "#     plt.ylim(0,max(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts()))\n",
    "    plt.plot(x,y,'r--')\n",
    "    plt.plot(x2,y2,'b-')\n",
    "    plt.xlabel(\"Dataframe: {0} Variable: tweets per {1}\".format(valuecountnames_all_data[k][1],valuecountnames_all_data[k][0]))\n",
    "    plt.ylabel(\"event type count\")\n",
    "\n",
    "\n",
    "\n",
    "    # Render the plot\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47564660",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxevent = max(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts())\n",
    "length = len(valuecountdfs_all_data2[k]['event'].get_group('not_informative'))\n",
    "print (\"Max: {} \\nOveral Numbers: {} \\n{}\".format(maxevent,length,(maxevent/length)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5af324",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxlist2=[]\n",
    "boxname=[]\n",
    "\n",
    "for k, df in enumerate(valuecountdfs_all_data):\n",
    "    boxlist2.append(valuecountdfs_all_data2[k]['event'].get_group('informative').value_counts().reset_index(drop=True).squeeze())\n",
    "    boxlist2.append(valuecountdfs_all_data2[k]['event'].get_group('not_informative').value_counts().reset_index(drop=True).squeeze())    \n",
    "    boxname.append(valuecountnames_all_data[k][1])\n",
    "    boxname.append(valuecountnames_all_data[k][1])\n",
    "plt.figure(figsize=(15, 5)) \n",
    "\n",
    "plt.boxplot(boxlist2) \n",
    "\n",
    "\n",
    "plt.ylabel(\"boxplot\")\n",
    "\n",
    "plt.xticks([1,2,3,4,5,6,7,8,9,10,11,12], boxname)\n",
    "#plt.ylim(5, 25)\n",
    "plt\n",
    "# boxname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c005b9de",
   "metadata": {},
   "source": [
    "### Analyis\n",
    "\n",
    "The counts of tweets in each event apear lognormally distributed, that is most of the the humanitarian crisis events in the combined and english dataframes have fewer than 10,000 tweets. While the absolute counts of tweets decreases from the Combined, through the the train, test and dev data sets, for the combined informativeness, and the informative/not-informative split the shape of the distributions appear consistent. \n",
    "\n",
    "The next phase will be to understand whether the counts of twitter events are influenced by the class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af4866",
   "metadata": {},
   "source": [
    "## Chi Squared Test\n",
    "\n",
    "Given that the events and informative variables are categorical, and appear to be log normally distributed, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08062d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2list=[]\n",
    "for l, df in enumerate(dflist):\n",
    "    x = dflist[l][dflist[l]['class_label']=='informative']['event']\n",
    "    y = dflist[l][dflist[l]['class_label']=='not_informative']['event']\n",
    "\n",
    "    print(stats.kruskal(x, y))\n",
    "    print(stats.mannwhitneyu(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e791680",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2list=[]\n",
    "for l, df in enumerate(dflist):\n",
    "    contingency = pd.crosstab(dflist[l]['event'], dflist[l]['class_label'])\n",
    "    chi2 = scipy.stats.chi2_contingency(contingency)\n",
    "    chi2list.append(chi2)\n",
    "    print(\"For {0}:\\n ChiSquared: {1}\\n p: {2}\\n Degreees of Freedom {3}\"\n",
    "          .format(valuecountnames_all_data[l][1],chi2[0],chi2[1],chi2[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b96514",
   "metadata": {},
   "source": [
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e0011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
