{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ff8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import datetime\n",
    "from nltk.tokenize import word_tokenize   \n",
    "import text_proccessing\n",
    "from text_proccessing import clean_text\n",
    "from text_proccessing import remove_stopwords\n",
    "from text_proccessing import lemmatize_text\n",
    "from text_proccessing import concatenate_text\n",
    "from text_proccessing import makeglove\n",
    "from text_proccessing import make_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585c90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(code_dir)\n",
    "data_dir = os.path.join(parent_dir,\"Data\")\n",
    "tweet_dir = os.path.join(parent_dir,\"TweetMap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5680f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar  1 10:40:16 2021\n",
    "\n",
    "@author: Arbo\n",
    "\"\"\"\n",
    "\n",
    "def twittsearch(text_query,since_date,until_date,tweetcount):\n",
    "    \n",
    "    tweets_list = []\n",
    "\n",
    "    query = '{0} since:{1} until:{2} filter:has_engagement'.format(text_query, since_date, until_date)\n",
    "    print(query)\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "\n",
    "        if i>tweetcount:\n",
    "\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.url,\n",
    "                            tweet.lang,tweet.retweetedTweet,tweet.quotedTweet])\n",
    "\n",
    "    # Creating a dataframe from the tweets list above\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, \n",
    "                             columns=['Datetime', 'TweetId', 'Text','TweetLink',\n",
    "                                     \"Language\", \"RTFrom\", \"QTFrom\"])\n",
    "    \n",
    "    print(\"found {} tweets ranging from {} to {}\".format(len(tweets_df),\n",
    "                                                            tweets_df.Datetime.min(),tweets_df.Datetime.max()))\n",
    "    \n",
    "    print(\"dropping duplicates\")   \n",
    "    tweets_df = tweets_df.drop_duplicates(subset=['TweetId'])\n",
    "    print(\"total of tweets now: {}\".format(len(tweets_df)))\n",
    "    print(\"english only\")\n",
    "\n",
    "    tweets_df = tweets_df[tweets_df[\"Language\"]=='en']\n",
    "    print(\"total of tweets now: {}\".format(len(tweets_df)))\n",
    "    tweets_df = tweets_df[tweets_df[\"RTFrom\"].isna()]\n",
    "    print(\"total of tweets now: {}\".format(len(tweets_df)))\n",
    "    tweets_df = tweets_df[tweets_df[\"QTFrom\"].isna()]\n",
    "    print(\"total of tweets now: {} ranging from {} to {}\".format(len(tweets_df),\n",
    "                                                                tweets_df.Datetime.min(),tweets_df.Datetime.max()))\n",
    "    \n",
    "    tweets_df = tweets_df[['Datetime', 'TweetId', 'Text','TweetLink']]\n",
    "    return tweets_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd932a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping commenced at 2021-07-17 15:23:29.888834\n",
      "(\"forest fire\") OR wildfire OR bushfire OR (extreme heat) OR (record heat) OR heatwave OR (\"heat wave\") OR typhoon OR cyclone OR hurricane OR tornado OR (\"storm surge\") OR  blizzard OR snow OR (\"ice storm\") OR sleet OR thunderstorm OR hail OR flood OR flooding OR freeze OR frost OR (extreme cold) OR landslide OR tsunami OR (\"tidal wave\") OR earthquake OR eruption OR volcano OR lava OR lahar OR avalanche OR mudslide OR sinkhole since:2021-06-26 until:2021-06-27 filter:has_engagement\n",
      "found 73977 tweets ranging from 2021-06-26 00:00:00+00:00 to 2021-06-26 23:59:57+00:00\n",
      "dropping duplicates\n",
      "total of tweets now: 73977\n",
      "english only\n",
      "total of tweets now: 44024\n",
      "total of tweets now: 44024\n",
      "total of tweets now: 41043 ranging from 2021-06-26 00:00:00+00:00 to 2021-06-26 23:59:57+00:00\n",
      "Scraping ended at 2021-07-17 15:37:04.473853\n",
      "Scraping time 0:13:34.585019\n"
     ]
    }
   ],
   "source": [
    "scrapestart =  datetime.datetime.now() \n",
    "print(\"Scraping commenced at {}\".format(scrapestart))\n",
    "# =============================================================================\n",
    "text_query = '(\"forest fire\") OR wildfire OR bushfire OR \\\n",
    "(extreme heat) OR (record heat) OR heatwave OR (\"heat wave\") OR typhoon OR cyclone OR hurricane OR \\\n",
    "tornado OR (\"storm surge\") OR  blizzard OR snow OR (\"ice storm\") OR sleet OR thunderstorm OR \\\n",
    "hail OR flood OR flooding OR freeze OR frost OR (extreme cold) OR landslide OR tsunami OR (\"tidal wave\") OR \\\n",
    "earthquake OR eruption OR volcano OR lava OR lahar OR avalanche OR mudslide OR sinkhole'\n",
    "\n",
    "\n",
    "# qlist = [firemetq_geologicalotherq]\n",
    "# qname= ['fire & meteorlogical & geological & other']\n",
    "since_date = '2021-06-26'\n",
    "until_date = '2021-06-27'\n",
    "tweetcount = 200000\n",
    "twts = twittsearch(text_query,since_date,until_date,tweetcount)\n",
    "# twittsearch(text_query,since_date,until_date,tweetcount).to_csv(os.path.join(tweet_dir,'tweets_df_{0}_{1}.csv'.\n",
    "#                               format(since_date.replace('-',''),until_date.replace('-',''))),\n",
    "#                  index =False)\n",
    "# =============================================================================\n",
    "scrapend =  datetime.datetime.now()\n",
    "print(\"Scraping ended at {}\".format(scrapend))\n",
    "print(\"Scraping time {}\".format(scrapend-scrapestart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49820a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing \n",
      "processed 0:01:54.239296\n"
     ]
    }
   ],
   "source": [
    "print(\"processing \")\n",
    "procstart =  datetime.datetime.now() \n",
    "twts['ptext'] = twts['Text'].apply(lambda x: clean_text(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x: word_tokenize(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : remove_stopwords(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : lemmatize_text(x))\n",
    "twts['ptext'] = twts['ptext'].apply(lambda x : concatenate_text(x))\n",
    "twts.to_csv(os.path.join(tweet_dir,\"processed_{}_{}.csv\".format(since_date.replace('-',''),until_date.replace('-',''))), index = False)\n",
    "procend=  datetime.datetime.now()\n",
    "\n",
    "\n",
    "print(\"processed {}\".format(procend - procstart))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d279eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
